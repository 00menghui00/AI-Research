# PaperBench: Evaluating AI’s Ability to Replicate AI Research
- 原文地址：https://arxiv.org/html/2504.01848?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/openai/preparedness/tree/main

## 介绍
- 为了实现客观评估，我们制定了评分标准，将每个复现任务按层次分解为更小的子任务，并设置明确的评分标准。PaperBench 总共包含 8,316 个可单独评分的任务。评分标准由我们与每篇 ICML 论文的作者共同制定，以确保准确性和真实性。（将任务分解、标准细化）
- 为了实现可扩展的评估，我们还开发了一个基于LLM的评判员，可以根据评分标准自动对复现尝试进行评分，并通过为评判员创建单独的基准来评估评判员的表现。（专用于打分的agent）


### 评判员
#### 引入
鉴于机器学习研究论文的复杂性，我们发现即使是对一次重复尝试进行评分，人类专家也可能需要数十个小时。为了简化评分流程，我们探索了基于 LLM 的评判系统，并引入了辅助评估工具 JudgeEval。
#### JudgeEval
- JudgeEval 是一个数据集，包含由人类专家评分的提交结果（即“黄金标准”）。
- 通过将自动化评判者（如 SimpleJudge）的输出与这些人类专家评分进行比较，JudgeEval 可以评估自动化评判者的准确性。
- 最好的基于LLM的评判者，使用了o3-mini-high和定制的框架，在辅助评估中取得了0.83的F1分数。


## 核心架构

### 1. PaperBench
#### 1.1 Task
- 对于 PaperBench 中的每个样本，被评估的智能体（ 候选者 ）都会收到论文及其说明附录。候选者必须提交一份包含代码库的提交 ，该代码库包含复现论文实证结果所需的所有代码。该代码库的根目录下必须包含一个 replicate.sh 文件，该文件作为执行所有必要代码以复现论文结果的入口点。如果提交的 replicate.sh 文件能够复现论文中报告的实证结果，则该提交的复现结果成功。
- 我们的数据集包含一些评分标准，这些标准定义了成功复现每份论文所需的具体结果。为了防止过度拟合评估标准，候选者在尝试过程中不会看到评分标准，候选者必须自行推断需要从论文中复现哪些内容。
- 重要的是，我们禁止代理使用或查看论文作者的原始代码库（如有）。这确保我们衡量的是代理从头开始编写代码并执行复杂实验的能力，而不是使用现有研究代码的能力。

#### 1.2 Reproduction

#### 1.3 Grading
- 基准中的每篇论文都有一个附带的评分标准，其中指定了完整论文复制的评估标准。
- 评分标准以树状结构组织，每个叶节点指定一个明确的通过或未通过标准，并且每个节点都根据其相对于兄弟节点的重要性进行了手动加权。给定一个叶节点标准，评委将评估提交内容是否符合其要求，如果符合则分配二进制分数 1，否则分配 0。
- 所有叶节点均完成评分后，父节点将获得一个分数，该分数等于其子节点分数的加权平均值。该分数将一直传递到树的根节点，根节点的分数将作为本次提交的最终复制分数 。
- 每个提交都根据所有满足的评分标准要求的权重调整比例进行评分，其中 100% 对应于完美复制且满足所有叶节点要求。我们的主要指标是所有论文的平均复制分数 。

#### 1.4 Requirement Types
Each leaf node has one of three possible requirement types, which determines how it is graded.

叶节点可以是代码开发、执行或结果匹配，这决定了在根据该叶节点评分时向评委展示哪些文件。

- *结果匹配*叶节点评估执行提交是否包含复制论文中特定结果的证据。结果匹配节点通过查看 reproduce.sh 和 reproduce.log 以及复制步骤中创建或修改的任何文件来进行评分。
- *执行*叶节点在运行 reproduce.sh 脚本时，评估是否发生了特定的执行结果。鉴于结果匹配节点尤其难以实现，拥有多个关联的执行节点允许提交在即使相应的结果匹配节点未实现的情况下，也能因标记部分进展而获得分数。执行节点通过查看 reproduce.sh、reproduce.log 和源代码进行评估。
- *代码开发*叶节点评估候选人的源代码是否看似包含对某些需求的一个正确实现。代码开发节点向执行节点的实现授予部分分数；例如，一个提交可能编写了正确的代码，但在 reproduce.sh 中未能正确执行。

#### 1.5 Rules

#### 1.6 PaperBench Code-Dev
- 在 PaperBench 上运行完整评估在代理模型推理以及提供给代理的计算环境中都非常昂贵。为了提高可访问性，我们发布了一个简化的 PaperBench 版本，称为 PaperBench Code-Dev。PaperBench Code-Dev 将评估任务简化为仅代码开发，跳过了关注执行代码以验证结果是否复现的步骤。在评估过程中，我们跳过复现步骤，并且评分标准仅对“代码开发”节点进行评分。
- PaperBench Code-Dev 提供了一种更易于获取，但不够稳健的评估代理论文复制能力的手段。我们发现其在 PaperBench Code-Dev 上的表现与在完整 PaperBench 评估上的表现呈微弱相关性。我们预期 PaperBench Code-Dev 可作为 PaperBench 表现的初步噪声指示。

### 2. Dataset 

#### 2.1 Rubric
- 每个评分标准都构建为树状结构，该结构按层次分解复制给定论文所需的主要成果。例如，根节点开始于最高级别的预期成果，例如“论文的核心贡献已被复制。”第一级分解可能为每个核心贡献引入一个节点。这些节点的子节点将更详细地描述具体成果，例如“gpt2-xl 已在数据集上微调，使用 B.1 节中的超参数。”重要的是，满足节点所有子节点表示父节点也已实现，因此只需评估树的所有叶节点，即可全面评估整体成功。
- 叶子节点具有精确和细粒度的要求。拥有许多细粒度要求使我们能够评分部分尝试，并使裁判更容易对单个节点进行评分。我们持续分解节点，直到它们所代表的要求足够细粒度，以至于我们估计一个熟悉该论文的专家可以在不到 15 分钟内审查提交内容是否满足该要求。
- 所有评分节点都具有权重；每个节点的权重表示其相对于其同级节点的贡献重要性，而不一定是该节点的实现难度。对节点进行加权可以奖励在复制时优先处理论文中更重要的部分。

#### 2.2 Dealing with Underspecification

### 3. LLM Judge
在初步实验中，我们发现人工评分需要耗费每篇论文数十小时，因此，为了 PaperBench 的实际应用，有必要开发一种自动化的评估方法。为了实现 PaperBench 提交的规模化评估，我们开发了一个基于 LLM 的简易裁判器（SimpleJudge）。接着，我们创建了一个辅助评估指标 JudgeEval，用于评估我们的裁判器以及未来裁判器的性能。

#### 3.1 SimpleJudge Implementation
- 给定一个提交，我们的评审员独立地根据评分标准对每个叶节点进行评分。对于特定的叶节点，评审员会收到论文的 Markdown 内容、完整的评分标准 JSON 文件、该叶节点的需求以及提交内容。
- 由于完整提交通常过长，无法完全适配模型的上下文，我们通过让裁判按相关性对代码库中的文件进行排序，仅包含排名前十的文件来过滤代码库，并将其置于上下文中。然后我们提示裁判评估叶节点的需求是否已满足。为了克服 LLM 上下文窗口的限制，并使其能够评估大型、复杂的代码提交，采用分而治之的策略：
 **信息筛选**： 通过一个预处理步骤，识别并只选择与当前评估任务最相关的少量（例如前十个）文件。
 **局部评估**： 让 LLM 评判者专注于评估这些相关文件中的特定、细粒度的“叶节点”要求。
- 除非另有说明，我们使用 OpenAI 的 o3-mini 作为裁判的后端模型。我们估计使用 o3-mini 对单个提交进行评分，在 OpenAI API 信用额度上大约需要 66 美元。对于 PaperBench Code-Dev，每篇论文的成本降至约 10 美元。我们的 LLM 裁判比雇佣专家人工评分要显著便宜和快速。

#### 3.2 Evaluating Judges with JudgeEval
JudgeEval，一个用于评估 PaperBench 环境下自动化裁判（SimpleJudge）准确性的基准。

- 为了构建 JudgeEval，我们从 PaperBench 数据集的四个论文和 PaperBench 开发集中的一个论文中使用了部分复制。这些复制要么从头开始创建，要么通过修改原始作者的代码库创建。我们手动根据相应论文的评分标准对每个复制尝试进行评分，并在评估自动裁判时将这些人工评级的叶节点作为真实标签。
- 由于对每个叶节点的评分是一个二分类任务，我们使用标准的二分类指标来评估 JudgeEval。**agent如何做分类？依靠LLM自身的能力和定制框架（提示词、工具调用、多智能体协作等方式）**

### 4. Experiments and Results

#### 4.1 Agent and Execution Environment
- 在我们的实验中，我们每个代理都在一个可以访问单个 A10 GPU 的 Ubuntu 24.04 Docker 容器中运行。代理的本地工作目录包含 PDF 和 Markdown 格式的论文、论文的附录以及包含说明的文本文件。（每个智能体都在自己的容器中运行，彼此之间不会相互干扰，也不会影响宿主机系统。这就像给每个智能体一个独立的“小电脑”。）
- 我们使用基于 Inspect AI 的基本代理的简单代理框架，称之为 BasicAgent，并使用 nanoeval 进行编排。该框架运行工具使用循环，直到模型选择终止运行或达到时间限制。我们为代理提供了一个 bash shell 命令执行工具、一个 Python 代码执行工具、一个网络浏览器工具和一个分页文件读取工具，用于读取长文档。

#### 4.2 Main Experiment
- 我们手动检查了部分代理日志，以更好地了解代理性能。我们观察到，除了 Claude 3.5 Sonnet 之外的所有模型都经常提前完成，声称它们要么已经完成了整个复制过程，要么遇到了无法解决的问题。所有代理都未能就如何在有限的时间内最佳地复制论文进行策略规划。我们观察到 o3-mini 经常在工具使用方面遇到困难。
- 这些失效模式表明当前模型在执行长时程任务方面存在缺陷；尽管在制定和撰写多步计划方面展现出丰富的能力，但模型却无法实际采取一系列行动来执行该计划。
- **我们相信，对智能体框架的进一步研究将有助于在 PaperBench 上取得更好的结果**。在我们的工作中，我们专注于介绍 PaperBench 基准，并仅将我们代理在该基准上的结果作为初始基线进行呈现。我们并不认为当前结果代表了这些模型能力的上限。
- IterativeAgent 防止模型提前结束任务，并提示模型以分步方式工作。我们观察到，与 BasicAgent 相比，这些修改显著提高了 o3-mini 和 o1 的分数，但损害了 Claude 3.5 Sonnet 的表现，突显了**模型对提示的敏感性**。
- 在 PaperBench 的 4 篇论文子集上比较人类与智能体的性能。o1 最初优于人类基线，但在第一个小时后达到平台期，导致其最终落后于人类。这表明该模型在尝试开始时擅长快速编写大量代码，但在这一时间范围之外无法有效工作以制定改进其提交的策略。人类的分数在最初几小时上升缓慢，可能是因为人类花费时间消化论文。

#### 4.3 IterativeAgent
- 鉴于模型往往无法充分利用其可用时间，我们测试了一种 BasicAgent 的变体，该变体通过移除代理提前结束任务的能力，强制代理运行其全部可用时间，并使用经过调整的提示来鼓励模型以分步方式工作。我们将这种代理称为 IterativeAgent。
- 我们观察到在使用 IterativeAgent 时，o1 和 o3-mini 的得分显著提升。我们注意到 Claude 3.5 Sonnet 在使用 BasicAgent 时表现优于 o1，但在使用 IterativeAgent 时表现劣于 o1。这表明 IterativeAgent 所使用的提示微调对 OpenAI o 系列模型具有不同的适配性。我们推测对 BasicAgent 进行修改，使其无法提前结束任务，可能会使 Claude 3.5 Sonnet 在使用 IterativeAgent 时表现优于 o1。

#### 4.4 Human Baseline Performance




