# PaperBench: Evaluating AI’s Ability to Replicate AI Research
- 原文地址：https://arxiv.org/html/2504.01848?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/openai/preparedness/tree/main

## 介绍
- 为了实现客观评估，我们制定了评分标准，将每个复现任务按层次分解为更小的子任务，并设置明确的评分标准。PaperBench 总共包含 8,316 个可单独评分的任务。评分标准由我们与每篇 ICML 论文的作者共同制定，以确保准确性和真实性。（将任务分解、标准细化）
- 为了实现可扩展的评估，我们还开发了一个基于LLM的评判员，可以根据评分标准自动对复现尝试进行评分，并通过为评判员创建单独的基准来评估评判员的表现。（专用于打分的agent）


### 评判员
#### 引入
鉴于机器学习研究论文的复杂性，我们发现即使是对一次重复尝试进行评分，人类专家也可能需要数十个小时。为了简化评分流程，我们探索了基于 LLM 的评判系统，并引入了辅助评估工具 JudgeEval。
#### JudgeEval
- JudgeEval 是一个数据集，包含由人类专家评分的提交结果（即“黄金标准”）。
- 通过将自动化评判者（如 SimpleJudge）的输出与这些人类专家评分进行比较，JudgeEval 可以评估自动化评判者的准确性。
- 最好的基于LLM的评判者，使用了o3-mini-high和定制的框架，在辅助评估中取得了0.83的F1分数。


## 核心架构

### 1. PaperBench
#### 1.1 Task
- 对于 PaperBench 中的每个样本，被评估的智能体（ 候选者 ）都会收到论文及其说明附录。候选者必须提交一份包含代码库的提交 ，该代码库包含复现论文实证结果所需的所有代码。该代码库的根目录下必须包含一个 replicate.sh 文件，该文件作为执行所有必要代码以复现论文结果的入口点。如果提交的 replicate.sh 文件能够复现论文中报告的实证结果，则该提交的复现结果成功。
- 我们的数据集包含一些评分标准，这些标准定义了成功复现每份论文所需的具体结果。为了防止过度拟合评估标准，候选者在尝试过程中不会看到评分标准，候选者必须自行推断需要从论文中复现哪些内容。
- 重要的是，我们禁止代理使用或查看论文作者的原始代码库（如有）。这确保我们衡量的是代理从头开始编写代码并执行复杂实验的能力，而不是使用现有研究代码的能力。

#### 1.2 Reproduction

#### 1.3 Grading
- 基准中的每篇论文都有一个附带的评分标准，其中指定了完整论文复制的评估标准。
- 评分标准以树状结构组织，每个叶节点指定一个明确的通过或未通过标准，并且每个节点都根据其相对于兄弟节点的重要性进行了手动加权。给定一个叶节点标准，评委将评估提交内容是否符合其要求，如果符合则分配二进制分数 1，否则分配 0。
- 所有叶节点均完成评分后，父节点将获得一个分数，该分数等于其子节点分数的加权平均值。该分数将一直传递到树的根节点，根节点的分数将作为本次提交的最终复制分数 。
- 每个提交都根据所有满足的评分标准要求的权重调整比例进行评分，其中 100% 对应于完美复制且满足所有叶节点要求。我们的主要指标是所有论文的平均复制分数 。

#### 1.4 Requirement Types
Each leaf node has one of three possible requirement types, which determines how it is graded.

叶节点可以是代码开发、执行或结果匹配，这决定了在根据该叶节点评分时向评委展示哪些文件。

- *结果匹配*叶节点评估执行提交是否包含复制论文中特定结果的证据。结果匹配节点通过查看 reproduce.sh 和 reproduce.log 以及复制步骤中创建或修改的任何文件来进行评分。
- *执行*叶节点在运行 reproduce.sh 脚本时，评估是否发生了特定的执行结果。鉴于结果匹配节点尤其难以实现，拥有多个关联的执行节点允许提交在即使相应的结果匹配节点未实现的情况下，也能因标记部分进展而获得分数。执行节点通过查看 reproduce.sh、reproduce.log 和源代码进行评估。
- *代码开发*叶节点评估候选人的源代码是否看似包含对某些需求的一个正确实现。代码开发节点向执行节点的实现授予部分分数；例如，一个提交可能编写了正确的代码，但在 reproduce.sh 中未能正确执行。

#### 1.5 Rules

#### 1.6 PaperBench Code-Dev
- 在 PaperBench 上运行完整评估在代理模型推理以及提供给代理的计算环境中都非常昂贵。为了提高可访问性，我们发布了一个简化的 PaperBench 版本，称为 PaperBench Code-Dev。PaperBench Code-Dev 将评估任务简化为仅代码开发，跳过了关注执行代码以验证结果是否复现的步骤。在评估过程中，我们跳过复现步骤，并且评分标准仅对“代码开发”节点进行评分。
- PaperBench Code-Dev 提供了一种更易于获取，但不够稳健的评估代理论文复制能力的手段。我们发现其在 PaperBench Code-Dev 上的表现与在完整 PaperBench 评估上的表现呈微弱相关性。我们预期 PaperBench Code-Dev 可作为 PaperBench 表现的初步噪声指示。

### 2. Dataset 

#### 2.1 Rubric
- 每个评分标准都构建为树状结构，该结构按层次分解复制给定论文所需的主要成果。例如，根节点开始于最高级别的预期成果，例如“论文的核心贡献已被复制。”第一级分解可能为每个核心贡献引入一个节点。这些节点的子节点将更详细地描述具体成果，例如“gpt2-xl 已在数据集上微调，使用 B.1 节中的超参数。”重要的是，满足节点所有子节点表示父节点也已实现，因此只需评估树的所有叶节点，即可全面评估整体成功。
- 叶子节点具有精确和细粒度的要求。拥有许多细粒度要求使我们能够评分部分尝试，并使裁判更容易对单个节点进行评分。我们持续分解节点，直到它们所代表的要求足够细粒度，以至于我们估计一个熟悉该论文的专家可以在不到 15 分钟内审查提交内容是否满足该要求。
- 所有评分节点都具有权重；每个节点的权重表示其相对于其同级节点的贡献重要性，而不一定是该节点的实现难度。对节点进行加权可以奖励在复制时优先处理论文中更重要的部分。

#### 2.2 Dealing with Underspecification

### 3. LLM Judge
在初步实验中，我们发现人工评分需要耗费每篇论文数十小时，因此，为了 PaperBench 的实际应用，有必要开发一种自动化的评估方法。为了实现 PaperBench 提交的规模化评估，我们开发了一个基于 LLM 的简易裁判器（SimpleJudge）。接着，我们创建了一个辅助评估指标 JudgeEval，用于评估我们的裁判器以及未来裁判器的性能。

#### 3.1 SimpleJudge Implementation
- 给定一个提交，我们的评审员独立地根据评分标准对每个叶节点进行评分。对于特定的叶节点，评审员会收到论文的 Markdown 内容、完整的评分标准 JSON 文件、该叶节点的需求以及提交内容。
- 由于完整提交通常过长，无法完全适配模型的上下文，我们通过让裁判按相关性对代码库中的文件进行排序，仅包含排名前十的文件来过滤代码库，并将其置于上下文中。然后我们提示裁判评估叶节点的需求是否已满足。为了克服 LLM 上下文窗口的限制，并使其能够评估大型、复杂的代码提交，采用分而治之的策略：
 **信息筛选**： 通过一个预处理步骤，识别并只选择与当前评估任务最相关的少量（例如前十个）文件。
 **局部评估**： 让 LLM 评判者专注于评估这些相关文件中的特定、细粒度的“叶节点”要求。
