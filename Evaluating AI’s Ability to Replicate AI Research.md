# PaperBench: Evaluating AI’s Ability to Replicate AI Research
原文地址：https://arxiv.org/html/2504.01848?_immersive_translate_auto_translate=1

## 介绍
- 为了实现客观评估，我们制定了评分标准，将每个复现任务按层次分解为更小的子任务，并设置明确的评分标准。PaperBench 总共包含 8,316 个可单独评分的任务。评分标准由我们与每篇 ICML 论文的作者共同制定，以确保准确性和真实性。（将任务分解、标准细化）
- 为了实现可扩展的评估，我们还开发了一个基于LLM的评判员，可以根据评分标准自动对复现尝试进行评分，并通过为评判员创建单独的基准来评估评判员的表现。（专用于打分的agent）


## 评判员
### 引入
鉴于机器学习研究论文的复杂性，我们发现即使是对一次重复尝试进行评分，人类专家也可能需要数十个小时。为了简化评分流程，我们探索了基于 LLM 的评判系统，并引入了辅助评估工具 JudgeEval。
### JudgeEval
- JudgeEval 是一个数据集，包含由人类专家评分的提交结果（即“黄金标准”）。
- 通过将自动化评判者（如 SimpleJudge）的输出与这些人类专家评分进行比较，JudgeEval 可以评估自动化评判者的准确性。
