# PaperBench: Evaluating AI’s Ability to Replicate AI Research
- 原文地址：https://arxiv.org/html/2504.01848?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/openai/preparedness/tree/main

## 介绍
- 为了实现客观评估，我们制定了评分标准，将每个复现任务按层次分解为更小的子任务，并设置明确的评分标准。PaperBench 总共包含 8,316 个可单独评分的任务。评分标准由我们与每篇 ICML 论文的作者共同制定，以确保准确性和真实性。（将任务分解、标准细化）
- 为了实现可扩展的评估，我们还开发了一个基于LLM的评判员，可以根据评分标准自动对复现尝试进行评分，并通过为评判员创建单独的基准来评估评判员的表现。（专用于打分的agent）


### 评判员
#### 引入
鉴于机器学习研究论文的复杂性，我们发现即使是对一次重复尝试进行评分，人类专家也可能需要数十个小时。为了简化评分流程，我们探索了基于 LLM 的评判系统，并引入了辅助评估工具 JudgeEval。
#### JudgeEval
- JudgeEval 是一个数据集，包含由人类专家评分的提交结果（即“黄金标准”）。
- 通过将自动化评判者（如 SimpleJudge）的输出与这些人类专家评分进行比较，JudgeEval 可以评估自动化评判者的准确性。
- 最好的基于LLM的评判者，使用了o3-mini-high和定制的框架，在辅助评估中取得了0.83的F1分数。


## 核心架构

### 1. PaperBench
#### 1.1 Task
- 对于 PaperBench 中的每个样本，被评估的智能体（ 候选者 ）都会收到论文及其说明附录。候选者必须提交一份包含代码库的提交 ，该代码库包含复现论文实证结果所需的所有代码。该代码库的根目录下必须包含一个 replicate.sh 文件，该文件作为执行所有必要代码以复现论文结果的入口点。如果提交的 replicate.sh 文件能够复现论文中报告的实证结果，则该提交的复现结果成功。
- 我们的数据集包含一些评分标准，这些标准定义了成功复现每份论文所需的具体结果。为了防止过度拟合评估标准，候选者在尝试过程中不会看到评分标准，候选者必须自行推断需要从论文中复现哪些内容。
- 重要的是，我们禁止代理使用或查看论文作者的原始代码库（如有）。这确保我们衡量的是代理从头开始编写代码并执行复杂实验的能力，而不是使用现有研究代码的能力。

#### 1.2 Reproduction

#### 1.3 Grading
- 基准中的每篇论文都有一个附带的评分标准，其中指定了完整论文复制的评估标准。
- 评分标准以树状结构组织，每个叶节点指定一个明确的通过或未通过标准，并且每个节点都根据其相对于兄弟节点的重要性进行了手动加权。给定一个叶节点标准，评委将评估提交内容是否符合其要求，如果符合则分配二进制分数 1，否则分配 0。
- 所有叶节点均完成评分后，父节点将获得一个分数，该分数等于其子节点分数的加权平均值。该分数将一直传递到树的根节点，根节点的分数将作为本次提交的最终复制分数 。
- 每个提交都根据所有满足的评分标准要求的权重调整比例进行评分，其中 100% 对应于完美复制且满足所有叶节点要求。我们的主要指标是所有论文的平均复制分数 。

#### 1.4 Requirement Types

