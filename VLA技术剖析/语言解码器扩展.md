---
---
---

“将语言模型解码器扩展为输出直接对应于车辆运动的物理动作标记”这句话，听起来很抽象，但其背后的技术实现主要依赖于两个关键步骤：**1. 动作量化与编码（创建码本）；2. 词汇表扩展与模型微调**。

下面我们来详细拆解这两个步骤。

---

### 步骤一：动作量化与编码 (Action Quantization and Encoding)

这是最关键的一步，目的是将**连续的、无限可能的**驾驶动作，转换成**离散的、有限数量的**“动作词汇”。这个过程就像把自然界中无限种颜色，归纳总结成一个包含 256 种标准颜色的“调色盘”。这个“调色盘”在专业术语里被称为 **码本 (Codebook)**。

具体技术如下：

#### 1. 定义动作空间 (Define the Action Space)

首先，需要确定用哪些物理量来描述一个驾驶动作。通常，一个短时间内的驾驶动作可以由以下参数定义：
*   **横向控制**：方向盘转角 (Steering Angle) 或 曲率 (Curvature)
*   **纵向控制**：速度 (Speed) 或 加速度 (Acceleration)

例如，一个“动作”可以被定义为一个二维向量 `(v, ω)`，其中 `v` 是目标速度，`ω` 是目标角速度（代表转向）。

#### 2. 离散化连续动作 (Discretize the Continuous Actions)

现实世界中，速度和角速度都是连续的。为了让语言模型能处理，必须将它们离散化。这里最常用的技术是 **k-均值聚类 (k-means clustering)**。

*   **做法**：
    1.  **收集数据**：从海量的人类专家驾驶数据（如 nuPlan, Waymo 数据集）中，提取出成千上万个真实的 `(v, ω)` 动作向量。
    2.  **运行聚类**：使用 k-means 算法对这些数据点进行聚类。假设我们设定 `k=1024`，算法会自动找到 1024 个“**聚类中心**”。
    3.  **构建码本**：这 1024 个聚类中心，就构成了我们的“**动作码本**”或“**动作词典**”。每一个中心点（比如 `(v_i, ω_i)`）都代表一种典型的、有代表性的、物理上可行的驾驶微操作。

#### 3. 创建动作标记 (Create Action Tokens)

现在，我们有了一个包含 1024 个标准动作的码本。接下来，为每一个标准动作分配一个唯一的**ID**或**标记 (Token)**。

*   聚类中心 1 `(v_1, ω_1)`  ->  `[ACTION_0001]`
*   聚类中心 2 `(v_2, ω_2)`  ->  `[ACTION_0002]`
*   ...
*   聚类中心 1024 `(v_1024, ω_1024)` -> `[ACTION_1024]`

至此，我们就成功地把复杂的连续驾驶动作，变成了 1024 个离散的、像单词一样的“动作标记”。

---

### 步骤二：词汇表扩展与模型微调 (Vocabulary Expansion and Model Fine-tuning)

有了“动作词典”，下一步就是教会 VLM 模型认识并使用这些新“词汇”。

#### 1. 扩展模型的词汇表 (Extend the Model's Vocabulary)

*   **技术**：这是语言模型框架（如 Hugging Face Transformers）中的一个标准操作。
*   **做法**：
    1.  **加载预训练VLM**：首先加载一个预训练好的视觉语言模型。
    2.  **添加新标记**：调用 `tokenizer.add_tokens()` 之类的函数，将我们新创建的 1024 个动作标记（如 `[ACTION_0001]` 到 `[ACTION_1024]`）正式添加到模型的**分词器 (Tokenizer)** 词汇表中。
    3.  **调整模型嵌入层**：因为词汇表变大了，模型的**词嵌入层 (Token Embedding Layer)** 也需要相应地扩展。这个嵌入层负责将每个标记（ID）映射成一个高维向量。需要为这 1024 个新标记随机初始化新的嵌入向量。

#### 2. 准备训练数据 (Prepare Training Data)

现在需要创建新的训练样本，让模型学习在什么情况下该使用哪个动作标记。

*   **做法**：
    1.  取一条人类专家的驾驶轨迹（一连串连续的 `(v, ω)` 动作）。
    2.  对于轨迹中的每一个动作，在我们的“动作码本”中找到**最接近**的那个聚类中心。
    3.  用该聚类中心对应的“动作标记”来替换原始的连续动作。
    4.  这样，一条连续的轨迹就被转换成了一个**动作标记序列**，例如：`[ACTION_0015], [ACTION_0015], [ACTION_0018], [ACTION_0020], ...`。

#### 3. 进行监督微调 (Perform Supervised Fine-tuning)

最后，用准备好的新数据对扩展后的模型进行微调。

*   **输入**：路况图像 + 指令文本。
*   **目标输出 (Ground Truth)**：
    *   如果是带推理的数据，目标就是 `[推理文本] + [动作标记序列]`。
    *   如果是纯轨迹数据，目标就是 `[动作标记序列]`。
*   **训练过程**：模型会像学习语言一样，学习“看到这样的图像，就应该生成这样的动作标记序列”。因为动作标记本身就是物理可行的，所以模型学会生成这些标记，就等于学会了生成可行的轨迹。

### 总结

将解码器扩展的核心技术可以概括为：

1.  **k-means 聚类**：将无限的连续动作**量化**为有限的、离散的、可行的“动作原型”，构建出**动作码本 (Action Codebook)**。
2.  **词汇表扩展 (Vocabulary Expansion)**：在技术层面，将这些动作原型作为新的“单词”**添加**到预训练 VLM 的词汇表和嵌入层中。
3.  **监督微调 (Supervised Fine-tuning)**：通过学习“图像 -> 动作标记序列”的映射关系，教会模型如何像组织语言一样，**自回归地**生成一连串动作标记来构成最终的驾驶轨迹。

通过这一系列操作，AutoVLA 巧妙地将一个复杂的**运动规划问题**，转化成了一个语言模型最擅长的**序列生成问题**，从而实现了简洁而高效的端到端驾驶。
