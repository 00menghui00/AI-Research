# A Survey on LLM-as-a-Judge （评分LLM综述论文）
论文地址：https://arxiv.org/html/2411.15594?_immersive_translate_auto_translate=1

近期研究表明，LLM-as-a-Judge这一理念能够将自动方法的可扩展性与专家判断中发现的细致、上下文敏感的推理相结合（Zheng 等人，2023b；Wang 等人，2023c；Zhu 等人，2023a；Li 等人，2023b；Chen 等人，2024c）。此外，在适当的提示学习或微调（Khattak 等人，2023）下，LLMs 可能变得足够灵活，以处理多模态输入（Chen 等人，2024b）。这些优势表明，LLM-as-a-Judge 方法可能作为一种新颖且广泛适用的范例，用于解决复杂和开放式的评估问题。

## 1. In-Context Learning
要应用 LLM-as-a-Judge，评估任务通常使用情境学习方法来指定，这些方法提供指令和示例来指导模型的推理和判断。这个过程涉及两个关键方面：输入设计和提示设计。对于输入设计，需要考虑要评估的变量类型（如文本、图像或视频）、输入方式（例如单独、成对或批量）及其位置（例如在开头、中间或结尾）。对于提示设计，可以采用四种不同的方法，这些方法包括生成分数、解决真/假问题、进行成对比较和进行多项选择。

### 1.1 Generating scores
使用相应的分数来表示评估非常直观。然而，需要更仔细考虑的是用于评估的分数的性质和范围。分数可以是离散的，常见的范围如 1-3、1-5（Jones 等人，2024 年）或 1-10（Zhu 等人，2023a；Li 等人，2023b）。或者，它可以是连续的，范围从 0 到 1 或 0 到 100（Xiong 等人，2024 年）。最简单的方法是通过上下文来评分，设定分数范围和主要评分标准。例如，“请评价其回答的有用性、相关性、准确性、细节程度。每位助手在 1 到 10 的量表上获得一个总分，其中较高的分数表示更好的整体表现”（Zhu 等人，2023a）。稍微复杂一点的方法是提供更详细的评分标准。更复杂的评分情况可以是语言模型作为考官（Bai 等人，2023 年），它使用李克特量表评分函数作为绝对评估指标。评估者根据预定义的维度（包括准确性、连贯性、事实性和全面性）给给定的回答打分。每个维度都在 1 到 3 的量表上评分，从最差到最好。同时，要求评估者根据先前分配给 4 个维度的分数，提供一个 1 到 5 的总体评分。这个分数作为衡量答案整体质量的一个指标。

### 1.2 Solving Yes/No questions
- 一个是/否问题需要对给定陈述进行判断，仅关注其准确性。这类问题简单直接，只提供两种固定回答——是或否，真或假——没有任何额外的比较或选择。
- 此类评估常用于中间过程，为反馈循环创造条件。例如，它促进了自我优化循环，如 Reflexion（Shinn 等人，2023 年）所示，该模型通过生成语言自我反思来为未来的尝试提供有价值的反馈。在稀疏奖励信号的场景中，例如二元成功状态（成功/失败），自我反思模型利用当前轨迹和持久记忆来生成细致且具体的反馈。类似地，在自我改进环境中（Tian 等人，2024 年），可以使用 Yes/No 问题来评估自定义短语，如"需要修改"和"不需要修改"，从而促进进入下一循环。此外，这些评估常用于测试知识准确性，并评估陈述是否与既定事实一致（Sun 等人，2023c），例如：“给定一个问题及其相关的检索知识图谱三元组（实体、关系、实体），要求你回答这些三元组和你的知识是否足以回答该问题（是或否）。”

### 1.3 Conducting pairwise comparisons
- 两两比较是指比较两个选项并选择哪一个更优越或更符合特定标准。它涉及在两个选项之间做出决策，而不是在“是”或“否”之间进行判断。这种比较可以是主观的，也可以基于客观标准。这种评估是相对评估。两两比较通常用于对多个选项进行排序或确定优先级，其中通过在成对之间进行多次比较来识别更好的选择或建立等级。
- 两两比较是一种已建立的方法，对多个领域产生了显著影响（Qin 等人，2024a）。正如（Liu 等人，2024b）所指出的，在两两比较的背景下，LLM 和人类评估比基于分数的评估更为一致。大量研究表明，在位置一致性方面，两两比较评估优于其他评判方法（Zheng 等人，2023b；Liusie 等人，2024）。此外，两两比较可以通过使用高级排序算法（Qin 等人，2024a；Liu 等人，2024b）、数据过滤（Yuan 等人，2023b）等方法扩展到更复杂的基于关系评估框架，如列表比较。在两两比较评估中，作为评判者的 LLM 被提示选择能更好地回答当前问题的回答。为了适应平局的可能性，引入了多种选项模式。两选项模式要求评判者从两个给定选项中选择更好的回答。三选项模式引入了额外的选择，允许评判者在两个回答都不更优时表示平局。

### 1.4 Making multiple-choice selections
- 多项选择题涉及提供多个选项，而非进行成对比较的相对选择，也不是进行是非判断。评价者必须选择最恰当或正确的一个。与判断题相比，这种方法允许更广泛的回答范围，可以评估更深入的理解或偏好。

## 2. Model Selection
### 2.1 General LLM
- 为了通过 LLM-as-a-Judge 实现评估自动化，一种有效的方法是采用先进的语言模型，如 GPT-4（OpenAI，2023），而不是人工评估员（Zheng 等人，2023b）。研究表明，基于 GPT-4 的评估器的准确性高于专业人工评估员，在评估中表现出更高的一致性和稳定性。同时，如果所使用的通用 LLM 在指令遵循或推理能力方面存在局限，LLM-as-a-Judge 方法的有效性可能会受到显著影响。

### 2.2 Fine-tuned LLM
- 然而，依赖外部 API 进行评估可能会引发隐私泄露的考量，且 API 模型的不可透明性也挑战了评估的可重复性。因此，后续研究建议通过强调成对比较或评分的使用来优化专为评估设计的语言模型。例如，PandaLM（Wang 等人，2023c）基于 Alpaca 指令和 GPT-3.5 标注构建数据，然后微调 LLaMA-7B（Touvron 等人，2023a）作为评估模型。JudgeLM（Zhu 等人，2023a）从多样化的指令集和 GPT-4 标注中构建数据，并微调 Vicuna（Touvron 等人，2023b）作为可扩展的评估模型。Auto-J（Li 等人，2023b）在多种场景下构建评估数据以训练生成式评估模型，该模型可提供评估和批判性意见。Prometheus（Kim 等人，2023）定义了数千个评估标准，基于 GPT-4 构建反馈数据集，并微调细粒度评估模型。
- 微调判别模型的标准流程主要包括三个步骤。步骤 1：数据收集。训练数据通常由三个组成部分构成：指令、待评估对象和评估结果。指令通常来源于指令数据集，而评估结果可以来自 GPT-4 或人工标注。步骤 2：提示设计。提示模板的结构可以根据评估方案进行调整（见In-Context Learning章节）。步骤 3：模型微调。使用设计的提示和收集的数据，评估器的微调过程通常遵循指令微调范式（Ouyang 等人，2022 年）。模型接收一条指令以及一个或多个响应，以生成包含评估结果和可能解释的输出。
- 在微调后，评估模型可用于评估目标对象。虽然这些微调后的模型在自设计的测试集上通常表现出优越的性能，但它们在评估能力方面存在一些局限性。当前的提示和微调数据集设计往往导致评估 LLMs 泛化能力差，使其难以与 GPT-4 等强大的 LLMs 进行比较。

### 2.3 Post-processing Method
后处理细化了 LLM-as-a-Judge 生成的概率分布，以确保评估的准确性。评估格式应与我们的情境学习设计保持一致，并可能涉及增强提取评估可靠性的程序，这些程序应始终如一地应用。我们关注三种主要的后处理方法：提取特定标记、归一化输出 logits 以及选择高回报的句子。

然而，需要注意的是，在评估客观题时，每种方法都存在显著局限性。例如，在文本响应评估（Yu 等人，2024b）中，未能从 LLM 的响应中准确提取关键答案标记可能导致评估结果错误。这些后处理中的挑战与早期 ICL 阶段使用的提示设计以及所选模型可靠遵循指令的能力密切相关。

### 2.3.1 Extracting specific tokens

### 2.3.2 Constrained decoding
- 约束解码是一种通过根据预定义模式限制标记生成来强制大型语言模型（LLMs）产生结构化输出的技术，通常以 JSON 等格式实现。该方法使用有限状态机（FSM）在每次解码步骤中计算有效的下一个标记，有效地掩盖模型输出概率分布以确保符合所需模式。虽然这种方法保证了输出在语法上的有效性，但它也带来了一些挑战：可能扭曲模型学习到的分布并潜在地降低输出质量，需要大量的工程实现工作，并在推理过程中引入计算开销。
- 近期研究提出了多种解决方案以应对这些挑战。(Beurer-Kellner 等人，2024) 引入了 DOMINO，这是一种在保持自然分词的同时施加约束的解码算法。他们的系统通过预计算和推测解码来最小化开销，有时比无约束解码性能更快。(Dong 等人，2024b) 开发了 XGrammar，通过将标记分为可预检查的和需要运行时验证的，来加速语法约束生成。通过将语法引擎与 LLM 推理协同设计，他们实现了比现有方法快 100 倍的性能提升。(Zheng 等人，2024b) 提出了 SGLang，结合了特定领域的语言和优化的运行时。他们的系统具有高效的 KV 缓存重用和压缩的有限状态机，以实现更快的解码，证明了对编程模型和运行时的精心协同设计可以最小化约束解码的开销。

### 2.3.3 Normalizing the output logits

### 2.3.4 Selecting sentences


## Quick Practice

要有效地应用 LLM-as-a-Judge 设计，建议在测试周期中为各种场景寻找更有效的配置。使用 LLM-as-a-Judge 的成功也很大程度上取决于实现细节，包括任务复杂性、提示设计、所选模型以及后处理方法。LLM-as-a-Judge 快速实践过程包括四个主要阶段。首先是思考阶段，用户通过确定需要评估的内容、理解典型的人类评估方法以及识别一些可靠的评估示例来定义评估目标。接下来是提示设计，措辞和格式都很重要。最有效且通常有效的方法包括指定评分维度、强调相对比较以改进评估，并创建有效的示例来指导 LLM。第三阶段是模型选择，重点在于选择具有强大推理和指令跟随能力的大规模模型，以确保可靠的评估。 最后，标准化评估流程可确保输出结果的结构化。这可以通过使用特定格式（如\boxed{XX}、数值评分或二元响应（例如“是”或“否”）来实现。整个流程包括使用案例进行迭代测试，并通过重新测试进行完善，从而提高可靠性。在开发过程中，比较模型或提示并验证持续改进至关重要。
