# Tree of Thoughts: Deliberate Problem Solving with Large Language Models
原文地址：https://arxiv.org/html/2305.10601?_immersive_translate_auto_translate=1

## 概念梳理：

### 1. “语言模型在推理过程中仍然局限于令牌级、从左到右的决策过程”

#### 1. 令牌级 (Token-level)：
- 含义： 在大型语言模型中，文本不是以字符或单词为单位处理的，而是以“令牌”（Token）为单位。一个令牌可能是一个单词、一个词的一部分、一个标点符号，甚至是一个空格。例如，“Hello world!” 可能被分解为 ["Hello", " world", "!"] 三个令牌。
- 决策过程： LLM 的核心任务是预测下一个最有可能的令牌。它在每一步都根据到目前为止已经生成的所有令牌来计算下一个令牌的概率分布，然后从中选择一个（通常是概率最高的，或者通过采样选择）。

#### 2. 从左到右 (Left-to-right)：
- 含义： 这指的是模型生成文本的方向和顺序。LLM 是一种“自回归”（autoregressive）模型，这意味着它总是从序列的开头（左侧）开始生成，并逐步向后（右侧）生成。它生成一个令牌，然后将这个新生成的令牌添加到输入序列中，再预测下一个令牌，如此循环，直到生成结束符或达到最大长度。
- 决策过程： 每一次预测下一个令牌的“决策”，都是基于它前面已经生成的所有令牌。它无法“跳到”序列的中间或末尾去预先规划，也无法在生成过程中轻易地“回溯”到前面已经生成的令牌并修改它们。

#### 会导致如下问题：
##### 1. 缺乏全局规划和前瞻性：
- 人类在解决复杂问题时，通常会先进行高层次的规划，考虑最终目标，然后分解问题，甚至在必要时会进行“试错”和“回溯”。
- 而 LLM 的“从左到右”特性意味着它在生成每个令牌时，只能基于已有的上下文，无法提前“看到”整个推理路径或最终答案。这就像一个盲人摸象，每次只能摸到一部分，很难形成整体的认知。
- 这导致模型可能在推理初期就做出一个“局部最优”但“全局非最优”的决策，并且难以在后续步骤中纠正。
##### 2. 错误累积和传播：
- 由于模型是顺序生成，如果在一个早期步骤中预测了一个错误的令牌或做出了一个错误的推理，这个错误就会被带入后续的上下文，并可能导致后续的推理全部偏离，甚至产生“幻觉”。模型自身很难发现并纠正这些早期错误。
##### 3. 难以进行回溯和多路径探索：
- 对于需要探索多个可能性、进行条件判断、或者在发现死胡同后需要回溯重新选择路径的复杂推理任务（例如数学证明、编程调试、逻辑谜题），传统的从左到右生成方式效率低下且容易失败。模型无法像人类一样在脑海中“模拟”多条路径，并选择最佳路径。
##### 4. 效率问题：
- 即使是简单的推理，如果需要多步才能完成，模型也必须一步一步地生成。这在计算上可能效率不高，尤其是在需要大量中间步骤才能达到最终答案时。

