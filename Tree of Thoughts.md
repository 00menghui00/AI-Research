# Tree of Thoughts: Deliberate Problem Solving with Large Language Models
原文地址：https://arxiv.org/html/2305.10601?_immersive_translate_auto_translate=1

## 概念梳理：

### 1. “语言模型在推理过程中仍然局限于令牌级、从左到右的决策过程”

#### 1. 令牌级 (Token-level)：
- 含义： 在大型语言模型中，文本不是以字符或单词为单位处理的，而是以“令牌”（Token）为单位。一个令牌可能是一个单词、一个词的一部分、一个标点符号，甚至是一个空格。例如，“Hello world!” 可能被分解为 ["Hello", " world", "!"] 三个令牌。
- 决策过程： LLM 的核心任务是预测下一个最有可能的令牌。它在每一步都根据到目前为止已经生成的所有令牌来计算下一个令牌的概率分布，然后从中选择一个（通常是概率最高的，或者通过采样选择）。

#### 2. 从左到右 (Left-to-right)：
- 含义： 这指的是模型生成文本的方向和顺序。LLM 是一种“自回归”（autoregressive）模型，这意味着它总是从序列的开头（左侧）开始生成，并逐步向后（右侧）生成。它生成一个令牌，然后将这个新生成的令牌添加到输入序列中，再预测下一个令牌，如此循环，直到生成结束符或达到最大长度。
- 决策过程： 每一次预测下一个令牌的“决策”，都是基于它前面已经生成的所有令牌。它无法“跳到”序列的中间或末尾去预先规划，也无法在生成过程中轻易地“回溯”到前面已经生成的令牌并修改它们。

#### 会导致如下问题：
##### 1. 缺乏全局规划和前瞻性：
- 人类在解决复杂问题时，通常会先进行高层次的规划，考虑最终目标，然后分解问题，甚至在必要时会进行“试错”和“回溯”。
- 而 LLM 的“从左到右”特性意味着它在生成每个令牌时，只能基于已有的上下文，无法提前“看到”整个推理路径或最终答案。这就像一个盲人摸象，每次只能摸到一部分，很难形成整体的认知。
- 这导致模型可能在推理初期就做出一个“局部最优”但“全局非最优”的决策，并且难以在后续步骤中纠正。
##### 2. 错误累积和传播：
- 由于模型是顺序生成，如果在一个早期步骤中预测了一个错误的令牌或做出了一个错误的推理，这个错误就会被带入后续的上下文，并可能导致后续的推理全部偏离，甚至产生“幻觉”。模型自身很难发现并纠正这些早期错误。
##### 3. 难以进行回溯和多路径探索：
- 对于需要探索多个可能性、进行条件判断、或者在发现死胡同后需要回溯重新选择路径的复杂推理任务（例如数学证明、编程调试、逻辑谜题），传统的从左到右生成方式效率低下且容易失败。模型无法像人类一样在脑海中“模拟”多条路径，并选择最佳路径。
##### 4. 效率问题：
- 即使是简单的推理，如果需要多步才能完成，模型也必须一步一步地生成。这在计算上可能效率不高，尤其是在需要大量中间步骤才能达到最终答案时。

### 2. 语言模型推理框架“Tree of Thoughts”(ToT)

ToT推广了流行的“Chain of Thought”方法来提示语言模型，并支持探索作为解决问题的中间步骤的连贯文本单元（“思想”）。ToT 允许 LM 通过考虑多种不同的推理路径和自我评估选择来决定下一步行动，并在必要时向前看或回溯以做出全局选择，从而进行深思熟虑的决策。

### 3. “系统性消融”（Systematic Ablation）
#### 含义：
“消融”（Ablation）这个词来源于生物学，原指通过手术切除或破坏生物体的某个部分，以研究该部分的功能。在计算机科学中，它被引申为从一个完整的系统或模型中，有选择地移除（或禁用）一个或多个组件、模块、特征或机制，然后观察系统性能如何变化，以此来评估被移除部分对整体性能的贡献和重要性。
#### 系统性消融的主要目的是：
- 评估各组件的贡献度： 确定系统中每个独立部分（例如神经网络的某一层、某个注意力机制、某个特征工程步骤、LLM Agent 的某个模块）对最终性能的提升或下降有多大影响。
- 理解系统机制： 深入理解系统内部不同组件是如何协同工作，以及它们各自扮演的角色。
- 识别关键因素： 找出对系统性能至关重要的核心组件。
- 简化模型/系统： 如果某个组件被移除后对性能影响不大，那么它可能是冗余的，可以考虑移除以简化模型、提高效率或降低计算成本。
- 验证设计选择： 证明研究者在设计系统时所做的特定选择（例如引入某个新模块）是有效的和有意义的。
