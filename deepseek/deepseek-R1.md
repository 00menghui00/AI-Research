### DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

原文地址：https://arxiv.org/html/2501.12948?_immersive_translate_auto_translate=1


#### 重要概念：
1. Test‑time / Inference‑time Scaling：是指在推理阶段通过增加计算资源、并行/序列推理以及智能调度等方式，从而提升模型性能的一种灵活策略。它属于推理时扩展范畴，无需更大模型或更长训练，仅在运行时动态“多面思考”，被视为在当前模型发展遇瓶颈下，最具可行性和性价比的优化手段
2. Multi-stage training pipeline：是一种分段训练设计，从预训练到任务微调再到后期处理，沿着粗→精→特定方向优化，使模型能力全面而稳定
3. Cold-start：是指缺少历史交互信息的新用户或项目，需要借助侧信息、混合策略、数据增强、主动采样等方法来启动模型
4. SFT data（监督微调）：是专门为监督微调准备的高质量标签数据，用于指导预训练模型向特定任务进行定向训练。其显著特点是小而精、高质量，能快速提升模型专业能力和任务适配度
5. Checkpoint：是深度学习训练过程中的重要机制，能够在训练过程中保存模型的状态，防止数据丢失，支持恢复训练（防止意外中断后重新训练）和迁移学习（在其他数据集上微调）
6. Chain‑of‑Thought (CoT)：是一种通过让 LLM 显示“中间思路”，增强其多步推理能力的提示技术。在最新模型（如 OpenAI 的 o1/Strawberry）中，CoT 被训练成为模型的内部机制，无须显式提示即可“内部思考”
7. Rejection Sampling：在 LLM 强化训练（如 RLHF、DeepSeek‑R1）中，Rejection Sampling 用于从多次生成的候选中选取奖励最高的样本，并用这些优秀样本进行监督微调（如 SFT）
8. 生成式蒸馏（生成式Knowledge Distillation（KD））：用强模型生成样本 → 这些样本被视作“知识” → 小模型学习这些数据的输出 → 达到提升效果


#### 一些结论：
1. 我们证明了大型模型的推理模式可以蒸馏到小型模型中，其性能优于通过 RL 在小模型上发现的推理模式。开源的 DeepSeek-R1 及其 API 将有助于研究社区在未来蒸馏出更好的小型模型
2. 冷启动是训练初期帮助模型“暖身”的阶段，通过少量高质量示例引导模型学习目标结构或风格。它能显著提升学习效率、减少方差，并使后续强化训练更高效、更稳定，是现代 LLM 推理模型常用的训练策略之一。训练流程：（预训练 → 冷启动 SFT（监督微调）→ 强化训练（如 GRPO）→ 推理部署）

