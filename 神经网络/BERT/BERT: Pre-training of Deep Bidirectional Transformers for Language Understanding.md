# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)
- 论文地址：https://ar5iv.labs.arxiv.org/html/1810.04805?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/google-research/bert

## 概述：
1. BERT与GPT本质差异：BERT使用双向自注意力，可以捕捉完整信息。
