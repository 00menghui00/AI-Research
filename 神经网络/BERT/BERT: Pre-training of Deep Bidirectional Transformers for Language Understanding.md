# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)
- 论文地址：https://ar5iv.labs.arxiv.org/html/1810.04805?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/google-research/bert
