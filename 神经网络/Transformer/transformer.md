# Transformer
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1
- 文章讲解：https://www.ibm.com/think/topics/transformer-model

## 概述：
- Transformer 神经网络能够精细地辨别数据序列中每个部分如何影响和关联其他部分的能力，也赋予了它们许多多模态应用。Transformer 的核心特征是其自注意力机制，正是这一机制赋予了 Transformer 强大的能力，使其能够检测输入序列中各部分之间的关系（或依赖）。与它之前的 RNN 和 CNN 架构不同，Transformer 架构仅使用注意力层和标准的前馈层。
- 注意力机制则可以同时检查整个序列，并决定如何以及何时关注该序列的特定时间步。除了显著提高了理解长距离依赖的能力，Transformer 的这一特性还允许并行化：能够同时执行许多计算步骤，而不是以串行方式。（并行处理能力为Transformer大规模训练打开了机会）


## 注意力机制：
注意力机制本质上是一种算法，用于确定 AI 模型在任何特定时刻应该“关注”数据序列的哪些部分。自注意力机制的核心目的：当模型处理一句话中的某一个词时（比如 "apple"），自注意力机制能帮助模型判断，这句话里的其他哪些词对于理解 "apple" 这个词的当前含义最重要。

### 自注意力机制（Self-Attention）工作原理






## 自然语言理解：

聚焦于 **Word2Vec** 和 **Transformer** 这两种具体技术在“让计算机理解语言”上的方法，并对它们进行详细对比。

---

### **第一部分：Word2Vec 是如何让计算机理解语言的？**

Word2Vec 是“词嵌入”技术的开山之作，它的核心目标是为词汇表中的每一个词，学习一个固定不变的向量表示（即“词嵌入”）。它通过一个巧妙的代理任务（Proxy Task）来实现这一点，其背后是**分布式语义假设**。

**核心思想：** 一个词的意义由其上下文决定。

**技术实现（以 Skip-gram 模型为例）：**

1.  **目标：** 根据一个中心词，预测它周围的上下文词。
2.  **网络结构：** 一个非常简单的、只有三层的浅层神经网络。
    *   **输入层：** 中心词的独热编码（One-hot Encoding）。
    *   **隐藏层（投影层）：** 这一层没有激活函数，其权重矩阵就是我们最终想要的**词嵌入矩阵**。将输入层的独热编码与这个矩阵相乘，就相当于“查表”，得到了中心词的初始向量。
    *   **输出层：** 输出一个向量，其维度等于词汇表大小，并通过 Softmax 函数得到每个词作为上下文出现的概率。
3.  **学习过程：**
    *   模型接收一个中心词（如“国王”），并尝试预测其上下文（如“统治”、“王国”、“君主”等）。
    *   将预测结果与真实的上下文进行比较，计算误差。
    *   通过反向传播，根据误差来微调**隐藏层的权重矩阵（词嵌入矩阵）**。
4.  **最终产出：**
    *   当模型在海量文本上训练完成后，我们不再关心它的预测任务，而是直接取用那个经过充分学习的**隐藏层权重矩阵**。这个矩阵就是包含了所有词向量的“词嵌入表”。
    *   在这个表中，拥有相似上下文的词（如“国王”和“女王”）的向量，在数学上会非常接近。

**Word2Vec 理解语言的方式总结：**

*   **静态表征 (Static Representation)：** 它为每个词生成一个**唯一的、固定的**向量。无论 "bank" 出现在什么句子中，它的向量表示都是一样的。
*   **浅层上下文 (Shallow Context)：** 它只考虑一个词紧邻的几个词（由“窗口大小”参数决定），无法理解更复杂的语法结构或长距离的依赖关系。
*   **无监督学习：** 它不需要人工标注数据，仅通过原始文本就能学习。
*   **核心产物：** 一个高质量的、可供下游任务直接使用的**“词向量字典”**。

---

### **第二部分：Transformer 是如何让计算机理解语言的？**

Transformer 的目标远比 Word2Vec 宏大。它不仅要理解词的**基础语义**，更要深刻理解一个词在**具体语境中的动态含义**。

**技术实现：**

1.  **起点：可训练的词嵌入层**
    *   与 Word2Vec 类似，Transformer 也有一个词嵌入层（查找表）。但关键区别在于，这个查找表的初始向量是随机的，并且它作为**模型整体的一部分**，在最终任务（如翻译、问答）的驱动下，与所有其他参数**一起被端到端地训练和优化**。

2.  **核心引擎：自注意力机制 (Self-Attention)**
    *   这是 Transformer 理解动态语境的**核心技术**。它让句子中的每一个词都能“看到”所有其他词，并计算出相互之间的“相关性”。
    *   **QKV 机制：**
        *   每个词的初始向量会被映射成三个功能向量：Query (Q, 我想找什么), Key (K, 我有什么), Value (V, 我的内容是什么)。
        *   通过计算当前词的 Q 与所有词的 K 的**点积**，得到注意力分数，衡量了词与词之间的匹配度。
        *   分数通过 **Softmax** 归一化为注意力权重。
        *   用权重对所有词的 V 进行**加权求和**，得到一个全新的、融合了全局上下文信息的词向量。

3.  **结果：动态的、上下文相关的词表示**
    *   经过一层或多层自注意力机制的处理后，同一个词在不同句子中的最终向量表示是**完全不同**的。
    *   在 "The bank is on the river." 中，"bank" 的最终向量会更多地融入 "river" 的信息。
    *   在 "I need to go to the bank." 中，"bank" 的最终向量则会更多地融入 "go to" 等信息。

**Transformer 理解语言的方式总结：**

*   **动态表征 (Dynamic/Contextual Representation)：** 它为句子中的每个词生成一个**随语境变化的**向量。这是其与 Word2Vec 最根本的区别。
*   **深层与全局上下文 (Deep & Global Context)：** 通过多层自注意力机制，它可以捕捉到非常长距离的、复杂的依赖关系，理论上可以覆盖整个输入序列。
*   **端到端学习：** 词的语义理解是服务于最终任务的，并在任务的驱动下不断优化，而非一个独立的预处理步骤。
*   **核心产物：** 一个能够根据输入文本，实时计算出其中每个词的**深度上下文向量**的强大模型。

---

### **第三部分：Word2Vec 与 Transformer 的对比**

| 对比维度 | **Word2Vec** | **Transformer** |
| :--- | :--- | :--- |
| **核心目标** | 生成**静态的、通用的**词向量。 | 生成**动态的、上下文相关的**词向量。 |
| **上下文范围** | **局部、固定窗口** (Shallow & Local) | **全局、覆盖整个序列** (Global & Deep) |
| **输出结果** | 一个**固定的词向量查找表**。一个词只有一个向量。 | 一个**能够处理文本的模型**。同一个词在不同句子中有不同的向量。 |
| **能否解决多义词** | **不能**。无法区分 "bank" 的不同含义。 | **能**。通过上下文，可以为多义词生成不同的、恰当的向量表示。 |
| **学习过程** | **独立的预训练步骤**，产出可复用的词向量文件。 | **端到端的一体化训练**，词嵌入的学习是整个模型学习的一部分。 |
| **模型复杂度** | **非常简单**，浅层神经网络。 | **非常复杂**，包含多头注意力、位置编码、残差连接等众多模块。 |
| **角色定位** | **“原材料供应商”**：提供高质量的词向量作为基础原料，供其他模型使用。 | **“总装工厂”**：自己完成从原材料（文字）到最终成品（深度理解）的全过程。 |

**一个比喻总结：**

*   **Word2Vec** 就像是编纂了一本非常高质量的**《标准词典》**。它告诉你每个词的基础释义，这个释义是固定不变的。当你遇到一个词，你去查这本词典。
*   **Transformer** 则像是一位博学多闻、精通语境的**语言学大师**。他不仅知道每个词的基础含义，更重要的是，当你给他一整句话时，他能告诉你这句话里每一个词**在此处的、精确的、独一无二的含义**。他不是在“查字典”，而是在进行实时的“深度阅读理解”。
