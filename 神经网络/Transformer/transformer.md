# Transformer
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1
- 文章讲解：https://www.ibm.com/think/topics/transformer-model

---

## 概述：
- Transformer 神经网络能够精细地辨别数据序列中每个部分如何影响和关联其他部分的能力，也赋予了它们许多多模态应用。Transformer 的核心特征是其自注意力机制，正是这一机制赋予了 Transformer 强大的能力，使其能够检测输入序列中各部分之间的关系（或依赖）。与它之前的 RNN 和 CNN 架构不同，Transformer 架构仅使用注意力层和标准的前馈层。
- 注意力机制则可以同时检查整个序列，并决定如何以及何时关注该序列的特定时间步。除了显著提高了理解长距离依赖的能力，Transformer 的这一特性还允许并行化：能够同时执行许多计算步骤，而不是以串行方式。（并行处理能力为Transformer大规模训练打开了机会）

---

## 注意力机制：
注意力机制本质上是一种算法，用于确定 AI 模型在任何特定时刻应该“关注”数据序列的哪些部分。自注意力机制的核心目的：当模型处理一句话中的某一个词时（比如 "apple"），自注意力机制能帮助模型判断，这句话里的其他哪些词对于理解 "apple" 这个词的当前含义最重要。

### 自注意力机制（Self-Attention）工作原理





---

## 自然语言理解：

聚焦于 **Word2Vec** 和 **Transformer** 这两种具体技术在“让计算机理解语言”上的方法，并对它们进行详细对比。

---

### **第一部分：Word2Vec 是如何让计算机理解语言的？**

Word2Vec 是“词嵌入”技术的开山之作，它的核心目标是为词汇表中的每一个词，学习一个固定不变的向量表示（即“词嵌入”）。它通过一个巧妙的代理任务（Proxy Task）来实现这一点，其背后是**分布式语义假设**。

**核心思想：** 一个词的意义由其上下文决定。

**技术实现（以 Skip-gram 模型为例）：**

1.  **目标：** 根据一个中心词，预测它周围的上下文词。
2.  **网络结构：** 一个非常简单的、只有三层的浅层神经网络。
    *   **输入层：** 中心词的独热编码（One-hot Encoding）。
    *   **隐藏层（投影层）：** 这一层没有激活函数，其权重矩阵就是我们最终想要的**词嵌入矩阵**。将输入层的独热编码与这个矩阵相乘，就相当于“查表”，得到了中心词的初始向量。
    *   **输出层：** 输出一个向量，其维度等于词汇表大小，并通过 Softmax 函数得到每个词作为上下文出现的概率。
3.  **学习过程：**
    *   模型接收一个中心词（如“国王”），并尝试预测其上下文（如“统治”、“王国”、“君主”等）。
    *   将预测结果与真实的上下文进行比较，计算误差。
    *   通过反向传播，根据误差来微调**隐藏层的权重矩阵（词嵌入矩阵）**。
4.  **最终产出：**
    *   当模型在海量文本上训练完成后，我们不再关心它的预测任务，而是直接取用那个经过充分学习的**隐藏层权重矩阵**。这个矩阵就是包含了所有词向量的“词嵌入表”。
    *   在这个表中，拥有相似上下文的词（如“国王”和“女王”）的向量，在数学上会非常接近。

**Word2Vec 理解语言的方式总结：**

*   **静态表征 (Static Representation)：** 它为每个词生成一个**唯一的、固定的**向量。无论 "bank" 出现在什么句子中，它的向量表示都是一样的。
*   **浅层上下文 (Shallow Context)：** 它只考虑一个词紧邻的几个词（由“窗口大小”参数决定），无法理解更复杂的语法结构或长距离的依赖关系。
*   **无监督学习：** 它不需要人工标注数据，仅通过原始文本就能学习。
*   **核心产物：** 一个高质量的、可供下游任务直接使用的**“词向量字典”**。

---

### **第二部分：Transformer 是如何让计算机理解语言的？**

### 1. 误解的来源 vs. 真实的过程

#### **普遍的误解：**

1.  **第一步（预处理）**：拿所有训练文本，先用 Word2Vec 或类似算法跑一遍，得到一个固定的、包含所有词关系的“词向量字典”。
2.  **第二步（模型训练）**：在训练 Transformer 时，每当遇到一个词，就去这个预先做好的“字典”里查出它的向量，然后送入模型。Transformer 在此基础上学习更复杂的上下文关系。

#### **真实的过程 (The Transformer Way)：**

1.  **初始化 (Initialization)**：在训练开始之前，Transformer 的**词嵌入层**（那个巨大的“词汇表-向量”查找表）里的所有向量都是**完全随机**的，或者用一些简单的统计方法进行初始化。在这一刻，模型对所有词的关系**一无所知**，“国王”和“苹果”的向量可能比“国王”和“女王”的向量更接近。
2.  **一体化端到端训练 (End-to-End Training)**：
    *   模型接收一批文本数据（比如“今天天气很[MASK]”）。
    *   它从那个**随机的**嵌入层中查出每个词的随机向量。
    *   这些随机向量经过位置编码后，流经整个 Transformer 网络（自注意力层等）。
    *   模型根据这些随机向量和混乱的计算，做出一个**同样是随机的**预测（比如猜 `[MASK]` 是“西瓜”）。
    *   **关键步骤**：将这个错误的预测与正确答案（“好”）进行比较，计算出巨大的**误差 (Loss)**。
    *   **反向传播 (Backpropagation)**：这个误差会像冲击波一样，从模型的最后一层一直传回到**最开始的词嵌入层**。
    *   **同步更新**：根据这个误差信号，模型中**所有部分**的参数都会被微调，**包括词嵌入层里的那些初始随机向量**。例如，系统会发现，为了让最终的预测更接近“好”，需要把“天气”和“很”的词向量调整得更具某种特征，同时也要调整自注意力层的权重等等。

3.  **涌现 (Emergence)**：这个“随机猜测 -> 计算误差 -> 全局微调”的过程，在亿万个样本上重复亿万次之后，神奇的事情发生了：
    *   词嵌入层里的向量**不再是随机的**。为了让整个模型的最终预测任务做得更好，这些向量被“逼迫”着去学习和编码词语的语义信息。
    *   拥有相似上下文的词（如“国王”和“女王”），它们的向量被**自动地、逐渐地**推向了向量空间中的相近位置。
    *   同时，自注意力层也学会了如何利用这些逐渐变得有意义的词向量，来捕捉更复杂的上下文依赖。

---

### 2. Transformer 方式与 Word2Vec 方式的核心区别

| 特性 | **Word2Vec (独立预训练)** | **Transformer (端到端一体化训练)** |
| :--- | :--- | :--- |
| **学习目标** | **单一、浅层**：只专注于一个代理任务，即“根据中心词预测上下文”（或反之）。 | **最终、深层**：直接服务于整个模型的最终任务（如翻译、问答、遮盖词预测），目标更宏大。 |
| **学习过程** | **两阶段**：先训练词向量，再训练下游模型。词向量在第二阶段通常是**固定的 (Static)**。 | **一体化**：词向量的学习是整个模型学习过程的一部分，是**动态的 (Dynamic)**，会根据最终任务不断被优化。 |
| **上下文理解** | **局部上下文**：只考虑一个词周围的几个词（窗口大小有限）。 | **全局上下文**：通过自注意力机制，理论上可以考虑输入序列中的所有词，上下文信息更丰富。 |
| **结果** | 生成**“静态”**的词向量。一个词（如 "bank"）只有一个固定的向量，无法区分“银行”和“河岸”的含义。 | 生成**“上下文相关”**的词向量。经过 Transformer 处理后，同一个词 "bank" 在不同句子中的最终向量表示是不同的，因为它融合了全局上下文信息。 |
