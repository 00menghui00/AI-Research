# Attention Is All You Need
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1

## 概述：
- 我们提出了 Transformer，一种避免递归、完全依赖于注意力机制来建立输入和输出之间全局依赖关系的模型架构，Transformer 允许显著更多的并行化。
- 自注意力机制，有时也称为序列内注意力机制，是一种关联单个序列不同位置的注意力机制，用于计算序列的表示。



## encoder（编码器）
### 架构：
编码器由若干个相同的层堆叠而成，每层中包含两个子层，第一子层是多头自注意力机制，第二子层是简单的前馈神经网络，每个子层之间都通过残差连接并进行层归一化，即每一子层的输出都是 LayerNorm(x + Sublayer(x))，Sublayer(x) 是子层自身实现的函数,且每一子层的输出维度都是相同的。

## decoder（解码器）
### mask（掩码）：
解码器的任务是生成文本，这是一个自回归（Auto-regressive）的过程，掩码（Masking）的目的，就是在并行的、高效的训练模式下，强制模拟串行的、真实的生成过程，确保模型在预测位置 i 的词时，只能依赖位置 i 之前的信息。带掩码这个操作作用于自注意力机制计算注意力分数的环节。计算完注意力分数之后，构造一个与注意力分数矩阵同样大小的矩阵，然后加到计算出的注意力分数矩阵上，最后，对加了掩码的分数矩阵应用Softmax函数。经过softmax之后所有未来位置的注意力权重都变成了0。

### 架构：
解码器由若干个相同的层堆叠而成，每层中包含三个子层，第一子层是带掩码的多头自注意力，第二子层是编码器-解码器注意力，第三子层是前馈网络。每个子层之间连接方式与编码器一样。带掩码的自注意力层，负责处理已生成序列的内部关系（单向）。编码器-解码器注意力层，负责将已生成序列与编码器的输出进行对齐和信息提取（桥梁）。

## attention（注意力）
注意力函数可以被描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值的加权求和，其中分配给每个值的权重是通过查询与相应键的相似度函数计算得出的。
### Scaled Dot-Product Attention（缩放点积注意力）
- **Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V**
- Q = X * W_Q
- K = X * W_K
- V = X * W_V

#### Scaling（缩放）
为什么需要缩放？ 论文作者推测，当 d_k 的维度很大时，点积的结果的绝对值也可能会变得非常大。这会将Softmax函数推向其“饱和区”，在这个区域，梯度会变得极其微小，导致模型在反向传播时几乎无法学习（梯度消失）。通过除以 sqrt(d_k)，可以将点积结果的方差稳定在1左右，从而缓解这个问题，使训练过程更稳定。

### Multi-Head Attention（多头注意力）
多头注意力允许模型在不同位置同时关注来自不同表示子空间的信息。使用单个注意力头时，平均操作会抑制这一点。通过将每个头的维度降低（d_k = d_model / h），多头注意力的总计算量与原始的单头全维度注意力大致相当。

- MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W_O
- head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)
- Q_i = X * W_Q_i
- K_i = X * W_K_i
- V_i = X * W_V_i

单个注意力头只有三个权重矩阵（W_Q,W_K,W_V)，但是多头注意力有3*h个权重矩阵（多了h倍），可以捕捉到更多信息。


## Positional Encoding（位置编码）
由于我们的模型不包含递归和卷积，为了让模型能够利用序列的顺序，我们必须注入一些关于序列中 token 的相对位置或绝对位置的信息。为此，我们在编码器和解码器堆栈底部的输入嵌入中添加了"位置编码"。位置编码与词嵌入（语义信息）具有相同的维度，以便两者可以相加。
作者没有选择让模型自己去学习位置编码（虽然实验证明效果差不多），而是设计了一个固定的、基于三角函数的公式。
### 公式：
- PE(pos, 2i) = sin(pos / 10000^(2i / d_model))
- PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))
### 解读：
- pos是词在句子中的位置（0, 1, 2, ...）。
- i是向量的维度索引（0, 1, 2, ...）。
- 这个公式为每个位置pos生成一个d_model维的向量。向量的每个维度（由i决定）都对应一个不同频率的正弦或余弦波。偶数维度用sin，奇数维度用cos。
### 为什么选择三角函数？
- 唯一性：每个位置都有一个独一无二的位置编码向量。
- 外推性：即使模型在训练时只见过长度为100的句子，这个公式也能生成长度为1000的句子的位置编码，让模型有潜力处理比训练时更长的序列。
- 相对位置关系：这是最精妙的一点。对于任何固定的偏移量k，PE(pos+k)可以表示为PE(pos)的线性变换。这意味着，模型可以很容易地学习到词与词之间的相对位置关系。例如，模型可以通过一次线性变换，理解“向前看3个词”或“向后看2个词”这种相对概念，而不需要死记硬背每个绝对位置。这对于语言理解至关重要。
