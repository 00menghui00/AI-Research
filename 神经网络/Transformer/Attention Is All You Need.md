# Attention Is All You Need
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1

## 概述：
- 我们提出了 Transformer，一种避免递归、完全依赖于注意力机制来建立输入和输出之间全局依赖关系的模型架构，Transformer 允许显著更多的并行化。
- 自注意力机制，有时也称为序列内注意力机制，是一种关联单个序列不同位置的注意力机制，用于计算序列的表示。



## encoder（编码器）
### 架构：
编码器由若干个相同的层堆叠而成，每层中包含两个子层，第一子层是多头自注意力机制，第二子层是简单的前馈神经网络，每个子层之间都通过残差连接并进行层归一化，即每一子层的输出都是 LayerNorm(x + Sublayer(x))，Sublayer(x) 是子层自身实现的函数,且每一子层的输出维度都是相同的。

## decoder（解码器）
### mask（掩码）：
解码器的任务是生成文本，这是一个自回归（Auto-regressive）的过程，掩码（Masking）的目的，就是在并行的、高效的训练模式下，强制模拟串行的、真实的生成过程，确保模型在预测位置 i 的词时，只能依赖位置 i 之前的信息。带掩码这个操作作用于自注意力机制计算注意力分数的环节。计算完注意力分数之后，构造一个与注意力分数矩阵同样大小的矩阵，然后加到计算出的注意力分数矩阵上，最后，对加了掩码的分数矩阵应用Softmax函数。经过softmax之后所有未来位置的注意力权重都变成了0。

### 架构：
解码器由若干个相同的层堆叠而成，每层中包含三个子层，第一子层是带掩码的多头自注意力，第二子层是编码器-解码器注意力，第三子层是前馈网络。每个子层之间连接方式与编码器一样。带掩码的自注意力层，负责处理已生成序列的内部关系（单向）。编码器-解码器注意力层，负责将已生成序列与编码器的输出进行对齐和信息提取（桥梁）。

## attention（注意力）
注意力函数可以被描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值的加权求和，其中分配给每个值的权重是通过查询与相应键的相似度函数计算得出的。
### Scaled Dot-Product Attention（缩放点积注意力）
- **Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V**
- Q = X * W_Q
- K = X * W_K
- V = X * W_V

#### Scaling（缩放）
为什么需要缩放？ 论文作者推测，当 d_k 的维度很大时，点积的结果的绝对值也可能会变得非常大。这会将Softmax函数推向其“饱和区”，在这个区域，梯度会变得极其微小，导致模型在反向传播时几乎无法学习（梯度消失）。通过除以 sqrt(d_k)，可以将点积结果的方差稳定在1左右，从而缓解这个问题，使训练过程更稳定。

### Multi-Head Attention（多头注意力）
多头注意力允许模型在不同位置同时关注来自不同表示子空间的信息。使用单个注意力头时，平均操作会抑制这一点。通过将每个头的维度降低（d_k = d_model / h），多头注意力的总计算量与原始的单头全维度注意力大致相当。

- MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W_O
- head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)
- Q_i = X * W_Q_i
- K_i = X * W_K_i
- V_i = X * W_V_i

单个注意力头只有三个权重矩阵（W_Q,W_K,W_V)，但是多头注意力有3*h个权重矩阵（多了h倍），可以捕捉到更多信息。


## Positional Encoding（位置编码）
由于我们的模型不包含递归和卷积，为了让模型能够利用序列的顺序，我们必须注入一些关于序列中 token 的相对位置或绝对位置的信息。为此，我们在编码器和解码器堆栈底部的输入嵌入中添加了"位置编码"。位置编码与词嵌入（语义信息）具有相同的维度，以便两者可以相加。
作者没有选择让模型自己去学习位置编码（虽然实验证明效果差不多），而是设计了一个固定的、基于三角函数的公式。
### 公式：
- PE(pos, 2i) = sin(pos / 10000^(2i / d_model))
- PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))
### 解读：
- pos是词在句子中的位置（0, 1, 2, ...）。
- i是向量的维度索引（0, 1, 2, ...）。
- 这个公式为每个位置pos生成一个d_model维的向量。向量的每个维度（由i决定）都对应一个不同频率的正弦或余弦波。偶数维度用sin，奇数维度用cos。
### 为什么选择三角函数？
- 唯一性：每个位置都有一个独一无二的位置编码向量。
- 外推性：即使模型在训练时只见过长度为100的句子，这个公式也能生成长度为1000的句子的位置编码，让模型有潜力处理比训练时更长的序列。
- 相对位置关系：这是最精妙的一点。对于任何固定的偏移量k，PE(pos+k)可以表示为PE(pos)的线性变换。这意味着，模型可以很容易地学习到词与词之间的相对位置关系。例如，模型可以通过一次线性变换，理解“向前看3个词”或“向后看2个词”这种相对概念，而不需要死记硬背每个绝对位置。这对于语言理解至关重要。


## transformer数据流：

我们来梳理一下编码器和解码器输入端的具体数据流。

我们将以一个**英译中**的任务为例，假设要翻译的句子是：**"I am a student"** -> **"我 是 一个 学生"**。

---

### 1. 编码器 (Encoder) 的数据流

编码器的任务是理解输入句子 "I am a student"。

#### **第1步：分词 (Tokenization)**

*   输入文本被分割成一个个的“词元”（Token）。
*   **数据**：`["I", "am", "a", "student"]`

#### **第2步：词元转ID (Token to ID)**

*   根据一个预先建立好的词汇表（Vocabulary），将每个词元转换为其对应的唯一ID。
*   **数据**：`[101, 182, 170, 750]` (ID为示例)

#### **第3步：词嵌入 (Word Embedding)**

*   将每个ID输入到**编码器的输入嵌入层 (Encoder Input Embedding Layer)**。
*   这个嵌入层是一个巨大的权重矩阵（查询表），其行数等于词汇表大小，列数等于模型的维度 `d_model` (例如512)。
*   模型会根据ID查表，得到每个词元的初始向量表示。
*   **数据**：一个形状为 `(sequence_length, d_model)` 的矩阵，例如 `(4, 512)`。我们称之为 `X_embed`。
    *   `X_embed[0]` 是 "I" 的512维向量。
    *   `X_embed[1]` 是 "am" 的512维向量。
    *   ...

#### **第4步：位置编码 (Positional Encoding)**

*   由于Transformer本身无法感知顺序，我们需要为每个位置生成一个位置编码向量。
*   生成一个与 `X_embed` 同样形状 `(4, 512)` 的位置编码矩阵 `P`。
    *   `P[0]` 是位置0的编码向量。
    *   `P[1]` 是位置1的编码向量。
    *   ...
*   **数据**：位置编码矩阵 `P`。

#### **第5步：相加注入位置信息**

*   将词嵌入矩阵和位置编码矩阵**按元素相加**。
*   `Final_Input = X_embed + P`
*   **数据**：一个形状为 `(4, 512)` 的最终输入矩阵。现在，这个矩阵中的每个向量都**同时包含**了词的**语义信息**和**位置信息**。

#### **第6步：进入编码器栈 (Encoder Stack)**

*   将 `Final_Input` 矩阵送入第一个编码器层。
*   数据在这个由6层组成的编码器栈中自下而上传递。
*   每一层都会对输入的矩阵进行处理（自注意力、前馈网络等），并输出一个同样形状 `(4, 512)` 的更新后的矩阵。
*   **最终**，第6层编码器输出一个形状为 `(4, 512)` 的矩阵。这个矩阵是编码器对整个输入句子的**最终、最完整的理解**，我们称之为 `Encoder_Output`。这个 `Encoder_Output` 会被缓存起来，等待解码器使用。

---

### 2. 解码器 (Decoder) 的数据流

解码器的任务是根据 `Encoder_Output` 和已经生成的部分，来预测下一个中文词。这个过程是**自回归**的，我们以**训练时（Teacher Forcing模式）**的数据流为例，因为它更完整。

假设我们正在训练模型，目标是生成 "我 是 一个 学生"。

#### **第1步：准备解码器输入**

*   在目标句子前加上一个特殊的起始符 `<start>`，并去掉句末的词（或加上句末符`<end>`后去掉最后一个词）。
*   **解码器输入序列**：`["<start>", "我", "是", "一个"]`

#### **第2步：分词与ID转换**

*   与编码器类似，将这些中文词元转换为ID。
*   **数据**：`[2, 5, 10, 11]` (ID为示例，使用另一套中文词汇表)

#### **第3步：词嵌入 (Word Embedding)**

*   将这些ID输入到**解码器的输入嵌入层 (Decoder Input Embedding Layer)**。
*   **注意**：这是一个**独立于**编码器嵌入层的权重矩阵（但在Transformer论文中，这两个嵌入层与最后的Softmax层共享权重）。
*   同样，查表得到每个词元的向量表示。
*   **数据**：一个形状为 `(sequence_length, d_model)` 的矩阵，例如 `(4, 512)`。我们称之为 `Y_embed`。

#### **第4步：位置编码 (Positional Encoding)**

*   为解码器的输入序列生成相应的位置编码矩阵 `P'`，形状为 `(4, 512)`。
*   **数据**：位置编码矩阵 `P'`。

#### **第5步：相加注入位置信息**

*   `Final_Decoder_Input = Y_embed + P'`
*   **数据**：一个形状为 `(4, 512)` 的最终解码器输入矩阵。

#### **第6步：进入解码器栈 (Decoder Stack)**

*   将 `Final_Decoder_Input` 送入第一个解码器层。
*   在解码器层内部，数据流会更复杂：
    1.  **带掩码的自注意力**：`Final_Decoder_Input` 在这个子层中进行处理，以理解已生成序列的内部关系（单向）。
    2.  **编码器-解码器注意力**：上一步的输出作为**Query**，而之前缓存的 `Encoder_Output` 作为**Key**和**Value**，进行注意力计算。这是解码器“看”输入句子的关键步骤。
    3.  **前馈网络**：对融合了各种信息的结果进行最后的加工。
*   数据在6层解码器栈中自下而上传递，每一层都重复上述三个步骤。

#### **第7步：最终输出与预测**

*   第6层解码器输出一个最终的矩阵，形状为 `(4, 512)`。
*   这个矩阵经过最后的**线性变换层**和**Softmax函数**，转换为每个位置上对下一个词的**概率分布**。
*   **预测目标**：模型在训练时，其预测目标是向左偏移一位的目标序列 `["我", "是", "一个", "学生"]`。
    *   输入 `<start>` 时，模型应预测出 "我"。
    *   输入 `<start> 我` 时，模型应预测出 "是"。
    *   ...

### 总结图示

```
+-----------------------+                              +-----------------------+
|   Input Sentence      |                              |  Target Sentence      |
|  "I am a student"     |                              | "<start> 我 是 一个"  |
+-----------------------+                              +-----------------------+
           |                                                      |
           v                                                      v
+-----------------------+                              +-----------------------+
| Tokenize & ID Convert |                              | Tokenize & ID Convert |
+-----------------------+                              +-----------------------+
           |                                                      |
           v                                                      v
+-----------------------+                              +-----------------------+
|  Encoder Embedding    |                              |  Decoder Embedding    |
+-----------------------+                              +-----------------------+
           |                                                      |
           v                                                      v
+-----------------------+                              +-----------------------+
| Positional Encoding   |                              | Positional Encoding   |
+-----------------------+                              +-----------------------+
           | (Add)                                                | (Add)
           v                                                      v
+-----------------------+                              +-----------------------+
|   Encoder Stack (x6)  | --(Encoder_Output)-->        |   Decoder Stack (x6)  |
|                       |                              |  (Masked Attn, Enc-Dec Attn, FFN)
+-----------------------+                              +-----------------------+
                                                                  |
                                                                  v
                                                         +-----------------------+
                                                         | Linear & Softmax      |
                                                         +-----------------------+
                                                                  |
                                                                  v
                                                         +-----------------------+
                                                         | Predicted Probabilities|
                                                         | for "我 是 一个 学生" |
                                                         +-----------------------+
```

这个流程清晰地展示了编码器和解码器的输入**各自拥有独立的嵌入层和位置编码过程**，然后再进入它们各自的核心处理栈。
