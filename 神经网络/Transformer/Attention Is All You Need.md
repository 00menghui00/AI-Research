# Attention Is All You Need
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1

## 概述：
- 我们提出了 Transformer，一种避免递归、完全依赖于注意力机制来建立输入和输出之间全局依赖关系的模型架构，Transformer 允许显著更多的并行化。
- 自注意力机制，有时也称为序列内注意力机制，是一种关联单个序列不同位置的注意力机制，用于计算序列的表示。



## encoder（编码器）
### 架构：
编码器由若干个相同的层堆叠而成，每层中包含两个子层，第一子层是多头自注意力机制，第二子层是简单的前馈神经网络，每个子层之间都通过残差连接并进行层归一化，即每一子层的输出都是 LayerNorm(x + Sublayer(x))，Sublayer(x) 是子层自身实现的函数,且每一子层的输出维度都是相同的。

## decoder（解码器）
### mask（掩码）：
解码器的任务是生成文本，这是一个自回归（Auto-regressive）的过程，掩码（Masking）的目的，就是在并行的、高效的训练模式下，强制模拟串行的、真实的生成过程，确保模型在预测位置 i 的词时，只能依赖位置 i 之前的信息。带掩码这个操作作用于自注意力机制计算注意力分数的环节。计算完注意力分数之后，构造一个与注意力分数矩阵同样大小的矩阵，然后加到计算出的注意力分数矩阵上，最后，对加了掩码的分数矩阵应用Softmax函数。经过softmax之后所有未来位置的注意力权重都变成了0。

### 架构：
解码器由若干个相同的层堆叠而成，每层中包含三个子层，第一子层是带掩码的多头自注意力，第二子层是编码器-解码器注意力，第三子层是前馈网络。每个子层之间连接方式与编码器一样。带掩码的自注意力层，负责处理已生成序列的内部关系（单向）。编码器-解码器注意力层，负责将已生成序列与编码器的输出进行对齐和信息提取（桥梁）。

## attention（注意力）
注意力函数可以被描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值的加权求和，其中分配给每个值的权重是通过查询与相应键的相似度函数计算得出的。
### Scaled Dot-Product Attention（缩放点积注意力）
- **Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V**
- Q = X * W_Q
- K = X * W_K
- V = X * W_V

#### Scaling（缩放）
为什么需要缩放？ 论文作者推测，当 d_k 的维度很大时，点积的结果的绝对值也可能会变得非常大。这会将Softmax函数推向其“饱和区”，在这个区域，梯度会变得极其微小，导致模型在反向传播时几乎无法学习（梯度消失）。通过除以 sqrt(d_k)，可以将点积结果的方差稳定在1左右，从而缓解这个问题，使训练过程更稳定。

### Multi-Head Attention（多头注意力）
多头注意力允许模型在不同位置同时关注来自不同表示子空间的信息。使用单个注意力头时，平均操作会抑制这一点。通过将每个头的维度降低（d_k = d_model / h），多头注意力的总计算量与原始的单头全维度注意力大致相当。

- MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W_O
- head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)
- Q_i = X * W_Q_i
- K_i = X * W_K_i
- V_i = X * W_V_i

单个注意力头只有三个权重矩阵（W_Q,W_K,W_V)，但是多头注意力有3*h个权重矩阵（多了h倍），可以捕捉到更多信息。
