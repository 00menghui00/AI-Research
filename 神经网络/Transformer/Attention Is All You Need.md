# Attention Is All You Need
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1

## 概述：
- 我们提出了 Transformer，一种避免递归、完全依赖于注意力机制来建立输入和输出之间全局依赖关系的模型架构，Transformer 允许显著更多的并行化。
- 自注意力机制，有时也称为序列内注意力机制，是一种关联单个序列不同位置的注意力机制，用于计算序列的表示。



## encoder（编码器）
### 架构：
编码器由若干个相同的层堆叠而成，每层中包含两个子层，第一子层是多头自注意力机制，第二子层是简单的前馈神经网络，每个子层之间都通过残差连接并进行层归一化，即每一子层的输出都是 LayerNorm(x + Sublayer(x))，Sublayer(x) 是子层自身实现的函数,且每一子层的输出维度都是相同的。

## decoder（解码器）
### mask（掩码）：
解码器的任务是生成文本，这是一个自回归（Auto-regressive）的过程，掩码（Masking）的目的，就是在并行的、高效的训练模式下，强制模拟串行的、真实的生成过程，确保模型在预测位置 i 的词时，只能依赖位置 i 之前的信息。带掩码这个操作作用于自注意力机制计算注意力分数的环节。计算完注意力分数之后，构造一个与注意力分数矩阵同样大小的矩阵，然后加到计算出的注意力分数矩阵上，最后，对加了掩码的分数矩阵应用Softmax函数。经过softmax之后所有未来位置的注意力权重都变成了0。

### 架构：
解码器由若干个相同的层堆叠而成，每层中包含三个子层，第一子层是带掩码的多头自注意力，第二子层是编码器-解码器注意力，第三子层是前馈网络。每个子层之间连接方式与编码器一样。带掩码的自注意力层，负责处理已生成序列的内部关系（单向）。编码器-解码器注意力层，负责将已生成序列与编码器的输出进行对齐和信息提取（桥梁）。

## attention（注意力）
注意力函数可以被描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值的加权求和，其中分配给每个值的权重是通过查询与相应键的相似度函数计算得出的。
### Scaled Dot-Product Attention（缩放点积注意力）
**Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V**
