# Attention Is All You Need
- 论文地址：https://arxiv.org/html/1706.03762?_immersive_translate_auto_translate=1

## 概述：
- 我们提出了 Transformer，一种避免递归、完全依赖于注意力机制来建立输入和输出之间全局依赖关系的模型架构，Transformer 允许显著更多的并行化。
- 自注意力机制，有时也称为序列内注意力机制，是一种关联单个序列不同位置的注意力机制，用于计算序列的表示。



## encoder（编码器）
### 架构：
编码器由若干个相同的层堆叠而成，每层中包含两个子层，第一子层是多头自注意力机制，第二子层是简单的前馈神经网络，每个子层之间都通过残差连接并进行层归一化，即每一子层的输出都是LayerNorm(x + Sublayer(x))，且每一子层的输出维度都是相同的。

## decoder（解码器）
### mask（掩码）
解码器的任务是生成文本，这是一个自回归（Auto-regressive）的过程，掩码（Masking）的目的，就是在并行的、高效的训练模式下，强制模拟串行的、真实的生成过程，确保模型在预测位置 i 的词时，只能依赖位置 i 之前的信息。带掩码这个操作作用于自注意力机制计算注意力分数的环节。计算完注意力分数之后，构造一个与注意力分数矩阵同样大小的矩阵，然后加到计算出的注意力分数矩阵上，最后，对加了掩码的分数矩阵应用Softmax函数。经过softmax之后所有未来位置的注意力权重都变成了0。
