# OpenVLA: An Open-Source Vision-Language-Action Model
- 论文地址：https://arxiv.org/html/2406.09246?_immersive_translate_auto_translate=1
- 代码仓库：https://github.com/openvla/openvla

## 引言：
- 传统机器人模型采用模块化集成架构，本文提出的OpenVLA使用端到端架构，直接将 VLMs 作为语言模型词汇表中的标记进行微调，以生成机器人动作。我们的实验评估表明，这种简单但可扩展的流程显著提升了性能和泛化能力，优于先前的通才型策略（将预训练组件（如语言嵌入或视觉编码器）与从头初始化的额外模型组件相结合，在策略训练过程中学习将它们“拼接”在一起）。
- 在 VLM 训练过程中，模型使用配对或交错来自各种互联网来源的视觉和语言数据进行端到端训练，目标是对下一个文本标记进行预测。

## 模型架构：
### vision encoder（视觉编码器）
用 DINOv2 + SigLIP 深度感知世界，将图像输入映射到若干“图像块嵌入”。
### projector（投影器）
将视觉信息无缝传递给大脑，接收视觉编码器的输出嵌入，并将它们映射到语言模型的输入空间。
### LLM backbone（LLM主干）
用 Llama 2 进行统一的思考和决策，直接输出行动指令。

## OpenVAL训练：
### Prismatic-7B VLM预训练

### 对预训练的VLM模型微调
- 任务重新定义：我们将动作预测问题表述为一个‘视觉-语言’任务，其中输入的观测图像和自然语言任务指令被映射到一系列预测的机器人动作。
- 动作离散化：为了使 VLM 的语言模型主干能够预测机器人动作，我们将动作表示为 LLM 输出空间中的离散标记，通过将连续的机器人动作映射到语言模型的分词器使用的离散标记。（语言模型只能输出离散的标记（就像字典里的一个个单词），而机器人的动作（如位置、角度）是连续的数值。）借鉴谷歌的 RT-1 模型的方法（由 Brohan 等人提出），我们将机器人动作的每个维度单独离散化为 256 个区间中的一个。
- 改进点：对于每个动作维度，我们将区间宽度设置为均匀划分训练数据中 1 st 和 99 th 分位数的动作之间的区间。使用分位数而不是 Brohan 等使用的最小-最大边界，使我们能够忽略数据中可能大幅扩展离散化区间并降低我们动作离散化有效粒度的异常动作。（我们得到了一个在绝大多数时间里都表现得非常精准、灵巧的机器人，代价仅仅是它在执行极少数超大范围动作时，可能会“慢一拍”或者“分几步走”。这个交换，无疑是极其划算的。）
