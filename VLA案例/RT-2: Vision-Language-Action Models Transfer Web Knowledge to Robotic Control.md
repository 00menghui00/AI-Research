# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
- 文章地址：https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/
- 论文地址：https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1

## 摘要：
- 我们的目标是使单个端到端训练模型既能学会将机器人观测映射到动作，又能享受来自网络语言和视觉语言数据的大规模预训练带来的好处。为此，我们提出在机器人轨迹数据和互联网规模的视觉语言任务（如视觉问答）上对最先进的视觉语言模型进行协同微调。
- 与其他方法不同，我们提出了一种简单通用的方法来实现这一目标：为了将自然语言响应和机器人动作统一到同一格式，我们将动作表示为文本标记，并像自然语言标记一样直接将其整合到模型的训练集中。我们将这类模型称为视觉语言动作模型（VLA），并实例化了一个此类模型的示例，我们称之为 RT-2。
-  我们的广泛评估（6k 评估试验）表明，我们的方法能够生成高性能的机器人策略，并使 RT-2 从互联网规模的训练中获得一系列涌现能力。这包括显著提高对新型物体的泛化能力、能够解释机器人训练数据中未出现的指令（例如将物体放置在特定的数字或图标上），以及能够根据用户指令进行初步推理（例如拿起最小或最大的物体，或距离另一个物体最近的物体）。
-  我们进一步证明，引入思维链推理使 RT-2 能够执行多阶段语义推理，例如确定要拿起哪个物体作为临时的锤子（一块石头），或为疲惫的人选择最适合的饮料（能量饮料）。

## 梳理
- RT-2 概述：我们将机器人动作表示为另一种语言，可以将其转换为文本标记，并与互联网规模的视觉语言数据集一起进行训练。在推理过程中，文本标记被解码为机器人动作，从而实现闭环控制。这使我们能够利用视觉语言模型的骨干和预训练来学习机器人策略，将它们的部分泛化能力、语义理解和推理能力迁移到机器人控制中。
