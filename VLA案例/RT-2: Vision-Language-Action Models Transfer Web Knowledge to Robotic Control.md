# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
- 文章地址：https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/
- 论文地址：https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1

## 文章贡献：
- RT-2 表明，视觉语言模型（VLMs）可以转化为强大的视觉语言行动（VLA）模型，通过结合 VLM 预训练和机器人数据，可以直接控制机器人。
- 
