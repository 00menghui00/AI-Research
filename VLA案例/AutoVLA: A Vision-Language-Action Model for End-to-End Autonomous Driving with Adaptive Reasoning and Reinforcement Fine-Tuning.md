# AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning
- 论文地址：https://arxiv.org/html/2506.13757?_immersive_translate_auto_translate=1

## 引言
- 自动驾驶系统通常采用模块化范式，将驾驶任务分解为不同的子模块，例如感知、预测和规划。虽然这种设计能够实现结构化开发，但它可能导致错误累积和模块间缺乏联合优化，从而造成性能次优。端到端的自动驾驶随着统一模型架构的兴起而备受关注，该架构将原始传感器输入直接映射到最终驾驶动作。
- 在本文中，我们提出了 AutoVLA，一种新型 VLA 模型，它将推理和行动生成统一于单一的自回归生成模型中，用于端到端自动驾驶。（与OpenVLA的思路一脉相承，采用端到端架构）
- AutoVLA 直接从原始视觉输入和语言指令进行语义推理和轨迹规划。我们将连续轨迹分词为离散可行的行动，从而能够直接集成到语言模型中。
- 我们将连续轨迹分词为离散可行的行动，从而能够直接集成到语言模型中。在训练过程中，我们采用监督微调方法，使模型具备双重思考模式：快速思考（仅轨迹）和慢速思考（增强的推理链）。为了进一步提升规划性能和效率，我们引入了基于组相对策略优化（GRPO）的强化微调方法，在简单场景中减少不必要的推理（实现自适应推理）。
- 一些模型直接使用 VLM 生成文本动作或航点，但这些输出可能是物理上不可行的（大模型不懂物理规律，动力学定律），并存在模式崩溃（模型有时候会偷懒）问题。
- 为了克服这些局限性，我们提出了 AutoVLA，这是一个端到端的自动驾驶框架，它将物理动作标记直接集成到预训练的视觉语言模型（VLM）主干中，从而能够直接学习自回归规划策略。
- 我们的统一架构无缝集成了推理和动作生成，允许在直接轨迹（动作）生成和 CoT 推理（规划动作再执行）之间进行自适应切换，具体实现方式是通过混合（混合两种数据）监督微调训练，利用轨迹数据和非 CoT 推理数据来赋予模型双重处理能力（快速和慢速思考）。
- 我们通过将 RFT 应用于优化场景推理和低级规划中的端到端 VLA 框架，推进了这一方向，并采用 GRPO 来加速收敛并增强训练稳定性（应用RFT使端到端的模型能进行底层的、精确到每个动作的轨迹规划）。

### AutoVLA 模型及其训练过程概述：
AutoVLA 使用预训练的小型视觉语言模型（VLM）作为骨干。该模型接收多视角相机流、系统提示、驾驶指令和车辆状态作为输入，并输出文本推理和物理动作标记。在策略微调（SFT）中，采用具有强大视觉理解能力的大型 VLM 模型收集推理数据，该数据与轨迹数据一起用于 SFT 训练 AutoVLA 模型（“教师-学生模型”策略）。在强化微调（RFT）中，我们利用 GRPO 训练模型，以改进与验证奖励函数的一致性，同时通过惩罚过度推理来实现自适应推理。

## AutoVLA核心架构
### 扩展语言解码器
将语言模型解码器扩展为输出直接对应于车辆运动的物理动作标记。这些标记设计为符合物理约束，并且可以可靠地转换为物理上可行的规划轨迹。

### 采用GRPO强化微调（RFT）策略
RFT 阶段使模型能够进行自适应推理并优化规划性能：
- 实现自适应推理 (Enable adaptive reasoning)：这是优化“思考”的效率。通过奖励机制（比如对不必要的推理进行惩罚），教会模型智能地判断何时需要“慢思考”（进行详细推理），何时应该“快思考”（直接输出动作），从而在保证安全的前提下，最大限度地提高决策效率。
- 优化规划性能 (Optimize planning performance)：这是优化“行动”的质量。通过奖励那些更安全、更平稳、更高效的轨迹，让模型的驾驶水平超越单纯的模仿，甚至可能发现比人类专家更优的驾驶策略。
