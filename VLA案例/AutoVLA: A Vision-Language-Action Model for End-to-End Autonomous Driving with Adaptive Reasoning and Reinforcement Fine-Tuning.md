# AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning
- 论文地址：https://arxiv.org/html/2506.13757?_immersive_translate_auto_translate=1

## 引言
- 自动驾驶系统通常采用模块化范式，将驾驶任务分解为不同的子模块，例如感知、预测和规划。虽然这种设计能够实现结构化开发，但它可能导致错误累积和模块间缺乏联合优化，从而造成性能次优。端到端的自动驾驶随着统一模型架构的兴起而备受关注，该架构将原始传感器输入直接映射到最终驾驶动作。
- 在本文中，我们提出了 AutoVLA，一种新型 VLA 模型，它将推理和行动生成统一于单一的自回归生成模型中，用于端到端自动驾驶。
- AutoVLA 直接从原始视觉输入和语言指令进行语义推理和轨迹规划。我们将连续轨迹分词为离散可行的行动，从而能够直接集成到语言模型中。
- 我们将连续轨迹分词为离散可行的行动，从而能够直接集成到语言模型中。在训练过程中，我们采用监督微调方法，使模型具备双重思考模式：快速思考（仅轨迹）和慢速思考（增强的推理链）。为了进一步提升规划性能和效率，我们引入了基于组相对策略优化（GRPO）的强化微调方法，在简单场景中减少不必要的推理（实现自适应推理）。
- 一些模型直接使用 VLM 生成文本动作或航点，但这些输出可能是物理上不可行的（大模型不懂物理规律，动力学定律），并存在模式崩溃（模型有时候会偷懒）问题。
- 为了克服这些局限性，我们提出了 AutoVLA，这是一个端到端的自动驾驶框架，它将物理动作标记直接集成到预训练的视觉语言模型（VLM）主干中，从而能够直接学习自回归规划策略。
- 我们的统一架构无缝集成了推理和动作生成，允许在直接轨迹（动作）生成和 CoT 推理（规划动作再执行）之间进行自适应切换，具体实现方式是通过混合（混合两种数据）监督微调训练，利用轨迹数据和非 CoT 推理数据来赋予模型双重处理能力（快速和慢速思考）。
- 
