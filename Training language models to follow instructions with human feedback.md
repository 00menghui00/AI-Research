# Training language models to follow instructions with human feedback
论文地址：https://ar5iv.labs.arxiv.org/html/2203.02155?_immersive_translate_auto_translate=1

------------
------------
------------

## 引入
- 扩大语言模型的规模并不必然使其更好地理解用户意图。例如，大型语言模型可能会生成不真实、具有攻击性或对用户毫无帮助的输出。换句话说，这些模型与用户意图并不一致。
- 我们专注于对齐语言模型的方法进行微调。具体来说，我们使用基于人类反馈的强化学习（RLHF；Christiano 等人，2017 年；Stiennon 等人，2020 年）来微调 GPT-3，使其遵循广泛的书面指令。这种技术使用人类偏好作为奖励信号来微调我们的模型。我们收集了一个数据集，其中包含在提交给 OpenAI API （主要是英语）的提示上以及一些标记员编写的提示上，人类编写的期望输出行为演示，并使用这些数据来训练我们的监督学习基线。接下来，我们收集了一个数据集，其中包含在更大的一组 API 提示上，我们模型输出之间的人类标记比较。然后，我们在这个数据集上训练一个奖励模型（RM），以预测我们的标记员更喜欢哪个模型输出。最后，我们使用这个 RM 作为奖励函数，并使用 PPO 算法（Schulman 等人，2017 年）来微调我们的监督学习基线，以最大化这个奖励。（监督学习、强化学习）
- 三步走：(1)监督微调（SFT），(2)奖励模型（RM）训练，以及(3)在奖励模型上通过近端策略优化（PPO）进行强化学习。
- 此过程使 GPT-3 的行为与特定人群（主要是我们的标注人员和研究人员）的明确偏好保持一致，而不是任何更广泛的“人类价值观”概念，我们将这些模型称为 InstructGPT。
- 在 RLHF 微调过程中，我们在某些公共 NLP 数据集上观察到与 GPT-3 相比的性能退化，特别是在 SQuAD（Rajpurkar 等人，2018 年）、DROP（Dua 等人，2019 年）、HellaSwag（Zellers 等人，2019 年）以及 WMT 2015 法语到英语翻译（Bojar 等人，2015 年）上。这是一个“对齐税”的例子，因为我们的对齐过程以牺牲我们可能关心的某些任务的性能为代价。通过将 PPO 更新与增加预训练分布对数似然度的更新（PPO-ptx）混合，我们可以在这些数据集上显著减少性能退化，同时不损害标注者偏好分数。
- InstructGPT 仍然可能无法遵循指令、编造事实、对简单问题给出冗长的含糊回答，或无法检测到具有错误前提的指令。

------------
------------
------------

## Methods and experimental details
### High-level methodology
#### 步骤1：收集演示数据，并训练一个监督策略。
我们的标注员在输入提示分布上提供所需行为的演示。然后我们使用监督学习在这个数据上微调预训练的 GPT-3 模型。
#### 步骤2：收集比较数据，并训练奖励模型。
我们收集了一个包含模型输出比较的数据集，其中标注者会指出对于给定输入，他们更喜欢哪个输出。然后我们训练一个奖励模型来预测人类偏好的输出。
#### 步骤3：使用 PPO 优化策略以匹配奖励模型。
我们使用 RM 的输出作为标量奖励。我们使用 PPO 算法（Schulman 等人，2017 年）微调监督策略以优化此奖励。

步骤 2 和 3 可以持续迭代；在当前最佳策略上收集更多比较数据，用于训练新的 RM，然后训练新的策略。在实践中，我们的大多数比较数据来自我们的监督策略，其中一些来自我们的 PPO 策略。

### Model
#### Supervised fine-tuning (SFT)（监督微调）
- 我们在使用监督学习的情况下，基于我们的标注器演示对 GPT-3 进行微调。我们进行了 16 个 epoch 的训练，使用了余弦学习率衰减，并设置了 0.2 的残差 dropout。我们根据验证集上的 RM 分数来选择最终的 SFT 模型。类似于 Wu 等人 (2021) 的发现，我们发现我们的 SFT 模型在 1 个 epoch 后在验证损失上过拟合；然而，我们发现增加训练 epoch 数量有助于提高 RM 分数和人类偏好评分，尽管存在这种过拟合现象。
- 过拟合之后继续训练看似违背了*传统机器学习*中“一旦过拟合就应停止训练”的基本原则。然而，这里的关键在于：大语言模型过拟合的“对象”和训练的“最终目标”不一致。因为模型过拟合的对象（与标准答案的“字面相似度”）不再是真正的目标。继续训练是为了在更高层次的目标——“人类偏好”和“回答质量”上达到更好的对齐，即使这会牺牲与标准答案的表面相似性。

#### Reward modeling (RM)（奖励建模）
从移除最终未嵌入层的 SFT 模型开始，我们训练了一个模型来接收提示和响应，并输出一个标量奖励。(把一个已经会写回答的模型，通过“脑改造手术”（替换最后一层），变成了一个不会写作、只会给别人的回答打分的“品味裁判”)在本文中，我们仅使用 6B RMs，因为这可以节省大量计算资源，并且我们发现 175B RM 训练可能不稳定，因此不太适合用作 RL 中的值函数。(模型轻量会大大提升效率，因为在强化学习阶段会不断调用RM。RM稳定性更重要，否则评分会时好时坏导致学习出现问题）
