# Training language models to follow instructions with human feedback
论文地址：https://ar5iv.labs.arxiv.org/html/2203.02155?_immersive_translate_auto_translate=1

------------
------------
------------

## 引入
- 扩大语言模型的规模并不必然使其更好地理解用户意图。例如，大型语言模型可能会生成不真实、具有攻击性或对用户毫无帮助的输出。换句话说，这些模型与用户意图并不一致。
- 我们专注于对齐语言模型的方法进行微调。具体来说，我们使用基于人类反馈的强化学习（RLHF；Christiano 等人，2017 年；Stiennon 等人，2020 年）来微调 GPT-3，使其遵循广泛的书面指令。这种技术使用人类偏好作为奖励信号来微调我们的模型。我们收集了一个数据集，其中包含在提交给 OpenAI API （主要是英语）的提示上以及一些标记员编写的提示上，人类编写的期望输出行为演示，并使用这些数据来训练我们的监督学习基线。接下来，我们收集了一个数据集，其中包含在更大的一组 API 提示上，我们模型输出之间的人类标记比较。然后，我们在这个数据集上训练一个奖励模型（RM），以预测我们的标记员更喜欢哪个模型输出。最后，我们使用这个 RM 作为奖励函数，并使用 PPO 算法（Schulman 等人，2017 年）来微调我们的监督学习基线，以最大化这个奖励。（监督学习、强化学习）

