# Training language models to follow instructions with human feedback
论文地址：https://ar5iv.labs.arxiv.org/html/2203.02155?_immersive_translate_auto_translate=1

------------
------------
------------

## 引入
- 扩大语言模型的规模并不必然使其更好地理解用户意图。例如，大型语言模型可能会生成不真实、具有攻击性或对用户毫无帮助的输出。换句话说，这些模型与用户意图并不一致。
- 我们专注于对齐语言模型的方法进行微调。具体来说，我们使用基于人类反馈的强化学习（RLHF；Christiano 等人，2017 年；Stiennon 等人，2020 年）来微调 GPT-3，使其遵循广泛的书面指令。这种技术使用人类偏好作为奖励信号来微调我们的模型。我们收集了一个数据集，其中包含在提交给 OpenAI API （主要是英语）的提示上以及一些标记员编写的提示上，人类编写的期望输出行为演示，并使用这些数据来训练我们的监督学习基线。接下来，我们收集了一个数据集，其中包含在更大的一组 API 提示上，我们模型输出之间的人类标记比较。然后，我们在这个数据集上训练一个奖励模型（RM），以预测我们的标记员更喜欢哪个模型输出。最后，我们使用这个 RM 作为奖励函数，并使用 PPO 算法（Schulman 等人，2017 年）来微调我们的监督学习基线，以最大化这个奖励。（监督学习、强化学习）
- 三步走：(1)监督微调（SFT），(2)奖励模型（RM）训练，以及(3)在奖励模型上通过近端策略优化（PPO）进行强化学习。
- 此过程使 GPT-3 的行为与特定人群（主要是我们的标注人员和研究人员）的明确偏好保持一致，而不是任何更广泛的“人类价值观”概念，我们将这些模型称为 InstructGPT。
- 在 RLHF 微调过程中，我们在某些公共 NLP 数据集上观察到与 GPT-3 相比的性能退化，特别是在 SQuAD（Rajpurkar 等人，2018 年）、DROP（Dua 等人，2019 年）、HellaSwag（Zellers 等人，2019 年）以及 WMT 2015 法语到英语翻译（Bojar 等人，2015 年）上。这是一个“对齐税”的例子，因为我们的对齐过程以牺牲我们可能关心的某些任务的性能为代价。通过将 PPO 更新与增加预训练分布对数似然度的更新（PPO-ptx）混合，我们可以在这些数据集上显著减少性能退化，同时不损害标注者偏好分数。
- InstructGPT 仍然可能无法遵循指令、编造事实、对简单问题给出冗长的含糊回答，或无法检测到具有错误前提的指令。



