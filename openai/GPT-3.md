# Language Models are Few-Shot Learners
论文地址：https://ar5iv.labs.arxiv.org/html/2005.14165?_immersive_translate_auto_translate=1

## 摘要
近年来 NLP 领域的主流范式，即“预训练-微调”模式。尽管预训练模型的架构本身是通用的（“任务无关”），但要让它在具体任务上表现出色，仍然需要大量的针对该任务的标注数据进行微调。这限制了模型的应用范围，因为很多任务没有足够多的标注数据。相比之下，人类通常只需要几个例子或简单的指令就能完成新的语言任务——而当前的 NLP 系统在这方面仍然面临很大的挑战。核心发现：仅仅通过大幅增加模型规模（参数量），就可以让模型在不进行微调的情况下，在少样本学习任务上表现出惊人的能力，甚至能与那些经过大量数据微调的模型相媲美。任务无关的少样本性能： 意味着模型不需要针对每个新任务进行专门的训练，只需通过输入几个示例或指令，就能理解并执行任务。
