# Let’s Verify Step by Step
- 论文地址：https://ar5iv.labs.arxiv.org/html/2305.20050?_immersive_translate_auto_translate=1

## 论文贡献
我们进行了自己的研究，发现过程监督（为每个中间推理步骤提供反馈）在训练模型解决具有挑战性的 MATH 数据集问题时，显著优于结果监督（为最终结果提供反馈）。此外，我们还表明，主动学习（在模型训练过程中，让模型“主动”挑选那些对提升自身性能最有价值的样本，由人工或其他“标注者”对这些样本打标，然后再将它们加入训练集，反复迭代）显著提高了过程监督的效率。

## 观点
- 结果监督奖励模型（ORMs）仅使用模型思维链的最终结果进行训练，而过程监督奖励模型（PRMs）则在每个思维链步骤中接收反馈。过程监督具有充分的理由，因为它提供了更精确的反馈，因为它指定了任何错误发生的确切位置。它还具有与 AI 对齐相关的几个优势：人类更容易解释，并且更直接地奖励模型遵循人类认可的思维链。在逻辑推理领域，使用结果监督训练的模型经常使用不正确的推理来得出正确的最终答案
