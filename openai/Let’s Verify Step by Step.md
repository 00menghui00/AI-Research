# Let’s Verify Step by Step
- 论文地址：https://ar5iv.labs.arxiv.org/html/2305.20050?_immersive_translate_auto_translate=1

## 论文贡献
我们进行了自己的研究，发现过程监督（为每个中间推理步骤提供反馈）在训练模型解决具有挑战性的 MATH 数据集问题时，显著优于结果监督（为最终结果提供反馈）。此外，我们还表明，主动学习（在模型训练过程中，让模型“主动”挑选那些对提升自身性能最有价值的样本，由人工或其他“标注者”对这些样本打标，然后再将它们加入训练集，反复迭代）显著提高了过程监督的效率。

## 观点
- 结果监督奖励模型（ORMs）仅使用模型思维链的最终结果进行训练，而过程监督奖励模型（PRMs）则在每个思维链步骤中接收反馈。过程监督具有充分的理由，因为它提供了更精确的反馈，因为它指定了任何错误发生的确切位置。它还具有与 AI 对齐相关的几个优势：人类更容易解释，并且更直接地奖励模型遵循人类认可的思维链。在逻辑推理领域，使用结果监督训练的模型经常使用不正确的推理来得出正确的最终答案。
- 没有简单的方法可以自动化过程监督。因此，我们依赖于人类数据标注者来提供过程监督，具体方法是标注模型生成解决方案中每一步的正确性。为了消除我们对昂贵人工反馈的依赖，我们使用一个大规模模型来监督小规模模型的训练。
- 为了使解析单个步骤更容易，我们训练生成器以换行符分隔的逐步格式生成解决方案。具体来说，我们针对 MATH 训练问题进行少量样本生成解决方案，筛选出那些得到正确最终答案的解决方案，并在该数据集上对基础模型进行单次微调。这一步并非旨在教生成器新技能，而仅仅是为了教生成器以所需格式生成解决方案。
