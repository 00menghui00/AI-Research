# Improving Language Understanding by Generative Pre-Training
- https://github.com/openai/finetune-transformer-lm/tree/master

## 概述：
- 预训练-微调（Pre-training and Fine-tuning）范式：我们证明，通过在一个多样化的未标记文本语料库上对语言模型进行生成式预训练，然后在每个特定任务上进行判别式微调，可以在这些任务上实现巨大提升。
- 与以往的方法相比，我们在微调过程中利用了任务感知的输入转换，以实现有效的迁移，同时只需对模型架构进行最小的改动（不用改动模型主体即可对不同类型的任务进行处理）。在迁移过程中，我们利用了源自遍历式方法（通过添加特殊符号，将多段、结构化的输入“压扁”成一个长序列。） 的任务特定输入适配，该方法将结构化文本输入作为单个连续的词元序列进行处理。
- 在本文中，我们探索了一种结合无监督预训练和有监督微调的半监督方法，用于语言理解任务。我们的目标是学习一个通用的表示，该表示只需少量适配即可迁移到广泛的任务中。
- 我们采用一个两阶段的训练程序。首先，我们在未标记数据上使用语言建模目标来学习神经网络模型的初始参数。随后，我们使用相应的监督目标将这些参数适配到目标任务上。（预训练+微调）
