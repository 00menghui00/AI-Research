# Improving Language Understanding by Generative Pre-Training
- https://github.com/openai/finetune-transformer-lm/tree/master

## 概述：
- 预训练-微调（Pre-training and Fine-tuning）范式：我们证明，通过在一个多样化的未标记文本语料库上对语言模型进行生成式预训练，然后在每个特定任务上进行判别式微调，可以在这些任务上实现巨大提升。
- 与以往的方法相比，我们在微调过程中利用了任务感知的输入转换，以实现有效的迁移，同时只需对模型架构进行最小的改动（不用改动模型主体即可对不同类型的任务进行处理）。在迁移过程中，我们利用了源自遍历式方法（通过添加特殊符号，将多段、结构化的输入“压扁”成一个长序列。） 的任务特定输入适配，该方法将结构化文本输入作为单个连续的词元序列进行处理。
- 在本文中，我们探索了一种结合无监督预训练和有监督微调的半监督方法，用于语言理解任务。我们的目标是学习一个通用的表示，该表示只需少量适配即可迁移到广泛的任务中。
- 我们采用一个两阶段的训练程序。首先，我们在未标记数据上使用语言建模目标来学习神经网络模型的初始参数。随后，我们使用相应的监督目标将这些参数适配到目标任务上。（预训练+微调）




## GPT-1架构：
GPT-1的核心架构非常简洁、优雅，它就是**一个堆叠了12层的、仅包含解码器（Decoder-Only）的Transformer模型**。

---

### 整体架构图

我们可以将GPT-1的架构想象成一个垂直的加工流水线：

```
+-------------------------------------------------+
|                最终输出与预测                   |
|           (Linear Layer + Softmax)              |
+-------------------------------------------------+
                        ^
                        |
+-------------------------------------------------+
|                                                 |
|       GPT-1 Block #12 (Transformer Decoder)     |
|                                                 |
+-------------------------------------------------+
                        ^
                        |
                      ... (堆叠)
                        |
                        ^
+-------------------------------------------------+
|                                                 |
|        GPT-1 Block #1 (Transformer Decoder)     |
|                                                 |
+-------------------------------------------------+
                        ^
                        |
+-------------------------------------------------+
|                  输入处理模块                     |
|      (Token Embedding + Positional Embedding)   |
+-------------------------------------------------+
                        ^
                        |
+-------------------------------------------------+
|                    输入文本                       |
|                (Token Sequence)                 |
+-------------------------------------------------+
```

现在，我们来详细拆解每个模块的作用。

---

### 1. 输入处理模块 (Input Processing Module)

这是数据进入模型的第一站，负责将人类可读的文本转换为模型能理解的数学表示。它由两个子模块组成：

#### **a. 词元嵌入 (Token Embedding)**

*   **作用**：将输入的每个单词（或子词，即Token）映射到一个高维的、包含其基础语义的向量。
*   **如何工作**：它是一个巨大的、可学习的查询表（权重矩阵），尺寸为 `(词汇表大小, 模型维度)`，在GPT-1中是 `(40000, 768)`。当一个词元ID输入时，它就从表中查出对应的768维向量。
*   **目的**：为模型提供每个词的初始“语义原料”。

#### **b. 位置嵌入 (Positional Embedding)**

*   **作用**：为输入序列中的每个位置创建一个唯一的向量，以告诉模型每个词的位置信息。
*   **如何工作**：GPT-1使用的是**可学习的**位置嵌入，而不是原始Transformer论文中的正弦/余弦函数。它也是一个查询表，尺寸为 `(最大序列长度, 模型维度)`，在GPT-1中是 `(512, 768)`。根据词元的位置（0, 1, 2...）查出对应的位置向量。
*   **目的**：解决Transformer架构本身无法感知顺序的问题，为模型注入至关重要的**位置感**。

**数据流**：将**词元嵌入向量**和**位置嵌入向量**按元素**相加**，得到最终的、既包含语义又包含位置信息的输入向量，然后送入第一个GPT-1 Block。

---

### 2. GPT-1核心块 (GPT-1 Block)

这是模型的主体，由**12个完全相同**的块垂直堆叠而成。每个块都是一个简化版的Transformer解码器层。它的任务是对输入的信息进行一次深度加工和提炼。

每个GPT-1 Block包含两个核心子层：

#### **a. 带掩码的多头自注意力 (Masked Multi-Head Self-Attention)**

*   **作用**：这是模型进行**上下文理解**的核心。它让序列中的每个词元都能“回顾”并“关注”包括它自己在内的、所有在它之前出现的词元，然后根据相关性大小，将这些历史信息加权融合到自己的表示中。
*   **关键特性**：
    *   **自注意力 (Self-Attention)**：信息交互发生在序列内部，自己关注自己。
    *   **多头 (Multi-Head)**：GPT-1使用了**12个注意力头**。这允许模型从12个不同的“角度”或“子空间”同时去审视上下文。例如，一个头可能关注语法，另一个头关注语义。这大大增强了模型的表达能力。
    *   **带掩码 (Masked)**：这是“解码器”架构的标志。通过一个“未来掩码”，强制模型在处理任何一个位置时，都**不能看到**未来的信息。这保证了模型在生成文本时是**单向的、自回归的**，与人类的思考和说话方式一致。
*   **目的**：在序列内部进行信息交互，构建一个**动态的、上下文感知**的词元表示。

#### **b. 逐位置前馈网络 (Position-wise Feed-Forward Network)**

*   **作用**：在自注意力层完成“信息融合”后，这个网络对**每个词元**的表示进行一次独立的、更深度的**非线性变换**。
*   **如何工作**：它是一个小型的、由两层全连接层组成的神经网络。`FFN(x) = max(0, x*W1 + b1)*W2 + b2`。这个网络在所有位置上是**共享**的，但对每个词元是**独立**应用的。在GPT-1中，它的内部维度是 `768 * 4 = 3072`。
*   **目的**：增加模型的**非线性表达能力**，可以看作是对注意力层输出结果的“深度加工”或“提炼”，帮助模型学习更复杂的特征模式。

**连接方式**：与标准Transformer一样，每个子层（自注意力和前馈网络）的周围都包裹着一个**残差连接 (Residual Connection)** 和一个**层归一化 (Layer Normalization)**，即 `LayerNorm(x + Sublayer(x))`。这确保了信息和梯度能够顺畅地在12层深的มี网络中流动，使训练过程非常稳定。

---

### 3. 最终输出与预测模块 (Final Output & Prediction)

这是流水线的最后一站，负责将模型最终的内部表示转换为人类可读的预测结果。

*   **作用**：将最后一个GPT-1 Block输出的、代表下一个词信息的向量，转换为在整个词汇表上的概率分布。
*   **如何工作**：
    1.  取最后一个Transformer块的**最后一个词元**的输出向量（768维）。
    2.  将其通过一个**线性层 (Linear Layer)** 进行变换。这个线性层的权重矩阵，巧妙地**复用了输入端的词元嵌入矩阵**（权重共享）。
    3.  变换后得到一个与词汇表大小相同（40000维）的向量，称为“logits”。
    4.  将logits输入一个**Softmax函数**，将其归一化为概率分布。
*   **目的**：得出词汇表中每个单词是下一个词的概率，概率最高的那个词就是模型的最终预测。

### GPT-1架构参数总结

| 参数 | 数值 | 描述 |
| :--- | :--- | :--- |
| **总层数 (n_layer)** | 12 | 堆叠了12个GPT-1 Block |
| **模型维度 (d_model / n_embd)** | 768 | 所有向量表示的维度 |
| **注意力头数 (n_head)** | 12 | 每个自注意力层有12个头 |
| **前馈网络内部维度** | 3072 | `768 * 4` |
| **词汇表大小 (n_vocab)** | 40,000 | 使用Byte Pair Encoding (BPE) |
| **最大序列长度 (n_ctx)** | 512 | 模型能处理的最长序列 |
| **总参数量** | 1.17亿 | |
