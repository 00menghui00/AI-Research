# Rethink框架
“Rethink 强提示”指在单一 Agent 框架下，通过精心设计的高质量 Prompt（包括角色设定、示例演示、链式思考指令等），使单一模型在带示例（demonstration）的场景中，其性能几乎能够匹配最优的多智能体讨论（Multi-Agent Discussion）方法。这种做法强调在 Prompt 层面投入，能以极低的架构复杂度实现与多 Agent 框架相当的推理效果。

- 论文：Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?

---
---
---

# Don’t Build Multi-Agents
博文地址：https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents

- “提示工程”是一个术语，指的是需要以适合 LLM 聊天机器人的理想格式编写你的任务。 “上下文工程”是这一步的下一级别。它是在动态系统中自动执行这项工作。它需要更多的细微差别，并且实际上是构建 AI 代理工程师的首要工作。
- 提出两个原则（Principles of Context Engineering）

## A Theory of Building Long-running Agents
- 当代理在长时间运行时必须保持可靠并维持连贯的对话时，你必须做某些事情来控制潜在的错误累积。否则，如果你不小心，事情很快就会崩溃。
- 大多数现实世界的任务都有许多层次的细微差别，所有这些都有可能被误解。你可能认为一个简单的解决方案是仅仅将原始任务作为上下文复制到子代理中。这样，他们就不会误解自己的子任务。但请记住，在一个真实的生产系统中，对话很可能是多轮的，代理可能不得不进行一些工具调用来决定如何分解任务，而且任何数量的细节都可能对任务的解释产生影响。

### Context Engineering（上下文工程）

共享上下文，并共享完整的代理跟踪记录，而不仅仅是单个消息

### Actions carry implicit decisions（行为隐含决策）

行为隐含着决策，而冲突的决策会导致不良后果

### 单线程线性代理
原则 1 和 2 至关重要，且极少有必要违反，因此你应该默认排除任何不遵守这些原则的智能体架构。你可能会认为这很限制，但实际上，你仍然可以探索许多不同的智能体架构。遵循原则最简单的方法就是只使用单线程线性代理。

### Applying the Principles
- 截至 2025 年 6 月，Claude Code 是一个会生成子任务的智能体。然而，它从不与子任务智能体并行工作，而且子任务智能体通常只被分配回答问题，而不是编写任何代码。为什么？子任务智能体缺乏来自主智能体的上下文，否则它将无法做任何超出回答一个明确定义的问题之外的事情。而且，如果它们运行多个并行子智能体，它们可能会给出冲突的响应，导致我们之前看到的智能体可靠性问题。在这种情况下，拥有一个子智能体的好处是，所有子智能体的调查工作都不必保留在主智能体的历史记录中，从而允许在上下文用尽之前运行更长的跟踪。Claude Code 的设计者采取了故意简单的方法。
- 2024 年，许多模型在编辑代码方面表现很差。在编码代理、IDE、应用构建器等（包括 Devin）中，一种常见的做法是使用“编辑应用模型”。其核心思想是，给定一个描述所需更改的 Markdown 说明，让一个小模型重写整个文件，实际上比让一个大模型输出格式正确的 diff 更可靠。因此，构建者让大模型输出代码编辑的 Markdown 说明，然后将这些 Markdown 说明输入给小模型以实际重写文件。然而，这些系统仍然存在很多错误。例如，小模型经常会误解大模型的指令，由于指令中最轻微的不明确性而做出错误的编辑。如今，编辑决策和应用的执行通常由单个模型在一个动作中完成。
- 自从 ChatGPT 推出不久后，人们就开始探索多个智能体相互协作以实现目标的想法[3][4]。虽然我对智能体之间长期协作的可能性持乐观态度，但很显然，到 2025 年，运行多个智能体进行协作只会导致脆弱的系统。决策最终变得过于分散，智能体之间无法充分共享上下文。目前，我看不到任何人专门致力于解决这个困难的跨智能体上下文传递问题。我个人认为，随着我们让单线程智能体在与人类沟通方面变得更好，这个问题将自然而然地得到解决。当这一天到来时，它将解锁更大的并行性和效率。
