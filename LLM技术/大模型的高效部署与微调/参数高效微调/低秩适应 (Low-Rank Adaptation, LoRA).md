# LoRA: Low-Rank Adaptation of Large Language Models(2021)
- 论文地址：https://ar5iv.labs.arxiv.org/html/2106.09685?_immersive_translate_auto_translate=1
- 博客介绍：https://www.ibm.com/think/topics/lora
- 知乎介绍：https://zhuanlan.zhihu.com/p/623543497


---
---

### 一、 LoRA原理总结：给大模型打上“小补丁”

LoRA（Low-Rank Adaptation，低秩适应）是一种**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**技术，其核心思想是在不改变大模型原有庞大参数的前提下，通过训练一个极小的“补丁”来实现对模型行为的微调。

#### 核心原理：

1.  **基本假设**：LoRA假设，当一个预训练好的大模型去适应一个新任务时，其权重矩阵需要做出的“改变”（`ΔW`）是**低秩（Low-Rank）**的。这意味着这个巨大的变化矩阵可以用两个非常小的、更“瘦”的矩阵（`A`和`B`）的乘积来近似表示，即 `ΔW ≈ B * A`。

2.  **冻结与注入**：
    *   **冻结（Freeze）**：在微调开始时，将原始大模型的所有权重（表示为 `W`）全部“冻结”，使其在训练过程中保持不变。这些权重占据了模型99.9%以上的参数。
    *   **注入（Inject）**：在模型需要微调的**密集层**（通常是Transformer中的QKV投影层和前馈网络层）旁边，注入两个可训练的小矩阵 `A` 和 `B`。`A` 的维度是 `r x d_in`，`B` 的维度是 `d_out x r`，其中 `r` 是一个非常小的数字，称为LoRA的“秩”（rank），是关键的超参数。

3.  **间接训练**：
    *   在训练时，我们**只更新**这两个小矩阵 `A` 和 `B` 的参数。由于 `r` 很小，`A` 和 `B` 的总参数量相比原始的 `W` 极小（通常不到0.1%）。
    *   模型进行前向计算时，输入 `x` 会同时经过两条路径：一条是原始路径 `W * x`，另一条是新增的LoRA“旁路” `(B * A) * x`。最终的输出是这两条路径结果的和：`Output = W * x + (B * A) * x`。
    *   因为 `W` 被冻结，损失函数的梯度只会用来更新 `A` 和 `B`。这就实现了用极小的计算和存储代价，来“间接”地调整整个密集层的功能。

**一句话比喻**：LoRA就像是给一台构造复杂且精密的庞大机器（预训练模型 `W`）加装一个可调节的“外挂旋钮”（LoRA模块 `A` 和 `B`）。我们不去动机器内部复杂的齿轮，只通过调节这个小旋钮，就能改变整台机器的输出行为。

---

### 二、 LoRA训练后的推理方式

训练结束后，我们得到了一个**不变的原始大模型**和一个**训练好的、小巧的LoRA权重文件**（包含矩阵`A`和`B`）。此时，我们有两种主流的推理（Inference）部署方式：

#### 方式一：动态加载，保持分离（最灵活）

这是最常用、最能体现LoRA优势的方式。

*   **步骤**：
    1.  **加载基础模型**：首先，将巨大的、原始的预训练模型（如Qwen3-4B）加载到内存/显存中。
    2.  **加载LoRA权重**：然后，加载那个几十MB大小的LoRA权重文件。
    3.  **应用补丁**：程序会自动将LoRA权重（矩阵`A`和`B`）“附加”到基础模型对应的网络层上。
*   **计算方式**：推理时的计算公式与训练时相同：`Output = W * x + (B * A) * x`。
*   **优点**：
    *   **节省空间**：一个基础模型可以搭配多个不同的LoRA权重，用于不同任务，无需为每个任务都存一个完整的模型。
    *   **任务切换快**：可以在不重启服务、不重新加载大模型的情况下，动态地挂载、卸载或切换不同的LoRA权重，实现模型功能的秒级切换。
*   **适用场景**：需要同时提供多种微调后模型服务的场景；个人研究和实验；需要快速迭代和测试不同微调效果的场景。

#### 方式二：合并权重，一体化部署（最便捷）

在确定了最终的微调效果后，为了部署方便或追求极致的推理速度，可以选择将LoRA权重合并进基础模型。

*   **步骤**：
    1.  这是一个**一次性的离线操作**。
    2.  加载基础模型和LoRA权重。
    3.  执行“合并”命令，程序会计算 `ΔW = B * A`。
    4.  将计算出的 `ΔW` 加到原始权重 `W` 上，得到新的权重 `W_new = W + ΔW`。
    5.  将这个包含了 `W_new` 的**新模型**完整地保存下来。
*   **计算方式**：推理时，直接加载这个合并后的新模型。计算公式变回了标准的全连接层形式：`Output = W_new * x`。
*   **优点**：
    *   **部署简单**：得到的是一个标准的、独立的大模型，无需任何关于LoRA的特殊处理，方便分享和部署。
    *   **速度微升**：由于计算路径上少了一次矩阵乘法和一次加法，推理速度理论上会有一点点微小的提升。
*   **缺点**：
    *   **失去灵活性**：合并是不可逆的，无法再从 `W_new` 中分离出原始的 `W` 和LoRA权重。
    *   **存储成本高**：每有一个微调任务，就需要保存一个完整大小（几十GB）的新模型。
*   **适用场景**：最终生产环境的部署；将微调后的模型分享给他人使用。

