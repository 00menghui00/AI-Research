# Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity(2021)
- 论文地址：https://ar5iv.labs.arxiv.org/html/2101.03961?_immersive_translate_auto_translate=1

## 概述：
- 我们简化了 MoE 路由算法，并设计了通信和计算成本更低、更直观的改进模型。
- 我们提出的训练技术缓解了不稳定性问题，并且首次证明了大型稀疏模型可以用较低精度的（bfloat16）格式进行训练。
- Switch Transformers 的指导设计原则是以简单且计算高效的方式最大化 Transformer 模型的参数数量。（规模效应由openai提出，并证明扩大模型比扩充数据更有性价比）
- 我们的假设是，参数数量，无论总计算量如何，都是可单独扩展的重要维度。（我们可以将参数量 (N) 和 单样本计算量 (FLOPs/sample) 进行解耦 (decouple)。我们可以做到让参数量 (N) 疯狂增长，但处理每个样本（token）所需的计算量却保持不变。）
- 分布式训练：我们的工作重点在于 TPU 架构，但这类模型也可以在 GPU 集群上进行类似训练。在我们的分布式训练设置中，我们的稀疏激活层在不同的设备上分割唯一的权重。因此，模型的权重随着设备数量的增加而增加，同时每个设备上的内存和计算开销保持可控。

## MoE架构：
- Expert：现代基于 Transformer 的 MoE 架构（如 Switch Transformer, Mixtral）中，“专家” (Expert) 指的就是一个标准的前馈网络 (Feed-Forward Network, FFN)。
- 稀疏激活：对于每一个传入的 token，系统并不会激活并使用所有的专家 FFN，而是通过一个路由机制（Router），只选择一个或少数几个（例如，Switch Transformer 中只选1个，Mixtral 中选2个）最相关的专家 FFN 来参与计算。

### 针对FFN做稀疏激活的原因：
- 参数量占比高: 在一个标准的 Transformer 模型中，FFN 层的参数量通常占到总参数量的 2/3 甚至更多。因此，对 FFN 进行“稀疏化”是提升模型参数/计算比（即“性价比”）最有效的地方，能带来最大的收益。
- 计算的独立性: FFN 层对序列中的每个 token 是独立计算的。Token "A" 在 FFN 中的计算与 Token "B" 无关。这种天然的独立性使得为每个 token 单独选择专家的“路由”机制变得非常简单和高效，极易并行化。
- 功能上的合理性: 我们可以直观地将不同的专家 FFN 理解为模型学习到的不同“知识领域”或“处理技能”。例如，一个专家可能擅长处理语法结构，另一个专家擅长处理事实性知识，还有一个专家擅长处理代码逻辑。让路由器根据 token 的内容为其分配合适的专家，在功能上非常合理。

---
---

## **负载均衡损失 (Load Balancing Loss)** 

---

### **1. 核心目标：我们到底想解决什么问题？**

在 MoE 模型中，我们有一个“路由器”（Router），它决定每个输入的 token 应该由哪个“专家”（Expert）来处理。如果不加约束，路由器可能会变得“懒惰”或“偏心”，把大部分甚至所有 token 都交给它认为最强的少数几个专家。这会导致两个灾难性后果：

1.  **专家能力退化**: “冷门”专家因为长期接不到任务（token），无法得到训练和更新，其参数会逐渐退化，最终变成无用的“摆设”。整个 MoE 模型退化成一个更小的、非稀疏的模型，失去了规模优势。
2.  **计算资源浪费/溢出**: 在分布式训练中，每个专家通常被分配到不同的计算设备上。如果任务分配不均，“热门”专家的设备会过载（导致 token 被丢弃，即“溢出”），而“冷门”专家的设备则会闲置。这极大地降低了训练效率。

因此，**负载均衡损失的核心目标就是：通过施加一个“惩罚项”，迫使路由器学会将任务（tokens）尽可能均匀地分配给所有可用的专家。**

---

### **2. 定义与公式解析**

在 Switch Transformer 论文中，这个辅助损失 (`L_aux`) 的定义如下：

`L_aux = α * N * Σ (fi * Pi)`  (从 i=1 到 N)

我们将这个公式拆解成四个部分来理解：`α`, `N`, `fi`, 和 `Pi`。

#### **第一部分：`fi` - “事后诸葛亮”：衡量实际工作量**

*   **公式**: `fi = (1/T) * Σ 𝟙{argmax(p(x)) = i}` (对批次 `ℬ` 中的所有 token `x` 求和)
    *   `T`: 一个批次（batch）中的 token 总数。
    *   `p(x)`: 路由器为 token `x` 计算出的、在所有 `N` 个专家上的概率分布向量。
    *   `argmax(p(x))`: 找出概率最高的那个专家的索引。
    *   `𝟙{...}`: 指示函数 (Indicator Function)。如果 `{}` 内的条件为真，函数值为1；否则为0。
    *   `Σ 𝟙{...}`: 这部分实际上就是在**统计**整个批次中，**有多少个 token 的最高分专家是 `i`**。
*   **直观解释**: `fi` 计算的是**“被最终分配给专家 `i` 的 token 占总 token 数量的比例”**。它衡量的是每个专家**实际收到了多少活儿**。
*   **关键特性**: 因为 `argmax` 函数的存在，`fi` 是一个离散的、**不可微分的**值。梯度无法通过它直接流回路由器。

#### **第二部分：`Pi` - “事前意图”：衡量路由器的期望**

*   **公式**: `Pi = (1/T) * Σ pi(x)` (对批次 `ℬ` 中的所有 token `x` 求和)
    *   `pi(x)`: 路由器为 token `x` 计算出的、分配给专家 `i` 的具体概率值（`softmax` 的输出）。
    *   `Σ pi(x)`: 将整个批次中所有 token 对专家 `i` 的“好感度”（概率分）全部加起来。
*   **直观解释**: `Pi` 计算的是**“专家 `i` 在整个批次中获得的平均路由概率”**。它衡量的是路由器**意图上想给专家 `i` 多少活儿**，而不管这些活儿最终有没有因为容量限制等原因被丢弃。
*   **关键特性**: 因为 `pi(x)` 是 `softmax` 的直接输出，所以 `Pi` 是**完全可微分的**。这是梯度能够流回路由器的关键通道。

#### **第三部分：`Σ (fi * Pi)` - 点积：核心惩罚机制**

*   **目标**: 我们希望路由是**均匀的**。在最理想的情况下，每个专家都应该处理 `1/N` 的 token，并且获得 `1/N` 的平均路由概率。即，我们希望 `fi` 和 `Pi` 向量的所有元素都趋近于 `1/N`。
*   **数学原理 (重排不等式)**: 对于两个正数序列，当它们同序排列时，它们的点积最大；当它们逆序排列时，点积最小。当其中一个或两个序列为常数时，点积也相对较小。
*   **如何工作**:
    1.  模型的目标是**最小化** `L_aux`，也就是最小化 `Σ (fi * Pi)` 这个点积。
    2.  假设路由**不均衡**：路由器偏爱专家 `j`。那么 `Pj` 会很大，其他 `Pk` 会很小。同时，因为大部分 token 都被路由给了专家 `j`，所以 `fj` 也会很大，其他 `fk` 会很小。
    3.  在这种情况下，`f` 和 `P` 这两个向量是**高度“同序”的**。它们都在相同的索引 `j` 上取得高值。这会导致它们的点积 `Σ (fi * Pi)` **变得很大**。
    4.  为了减小这个点积，优化器会通过 `Pi` 的梯度来调整路由器参数，迫使 `P` 向量变得更“平坦”（即所有 `Pi` 值都趋向于 `1/N`）。
    5.  当 `P` 向量变得更平坦时，路由决策会变得更随机、更均匀，从而使得 `f` 向量（实际负载）也自然地趋向于平坦。
    6.  当 `f` 和 `P` 都变得平坦时，点积 `Σ (fi * Pi)` 就达到了一个较低的值，从而实现了负载均衡的目标。

**这个点积设计非常巧妙，它通过一个简单的数学运算，同时惩罚了“意图上的不公”和“结果上的不公”。**

#### **第四部分：`α * N` - 缩放系数：保持量级稳定**

*   `α` **(alpha)**: 这是一个超参数，用来**调节惩罚的强度**。它是一个权衡：
    *   `α` 太大：模型过度关注负载均衡，可能会牺牲主任务的性能。
    *   `α` 太小：惩罚力度不够，无法有效解决负载不均的问题。
    *   论文作者通过实验发现 `α = 0.01` 是一个很好的值。
*   `N` **(专家数量)**: 为什么要乘以 `N`？这是一个**归一化 (Normalization)** 技巧。
    *   我们来看一下在理想的均匀分布下，点积的值是多少：`Σ (fi * Pi) = Σ (1/N * 1/N) = N * (1/N^2) = 1/N`。
    *   可以看到，如果不做处理，这个损失项的值会随着专家数量 `N` 的增加而减小。这意味着，对于拥有更多专家的模型，这个损失的初始影响力会更小，这会使得超参数 `α` 的调整变得困难（对于不同数量的专家需要设置不同的 `α`）。
    *   通过**乘以 `N`**，理想情况下的损失值变成了 `N * (1/N) = 1`。
    *   这样，无论专家数量 `N` 是 8 还是 64，这个辅助损失项的量级都被稳定在了 1 附近。这使得超参数 `α` 变得**与专家数量无关**，大大简化了模型的设计和调试。

---



---
---


## **Switch Transformer 层处理一个 Token 的完整生命周期（前向 + 反向传播）**

假设我们正在处理一个 token `x`，并且模型预测的最终结果与真实标签之间产生了误差（Loss）。现在，这个误差信号需要从模型的输出层一路传回来，对路径上的所有参数进行调整。

---

#### **1. 接收 (Receive)**
*   **前向传播**: Switch Transformer 层接收来自上一层（例如自注意力层）的输出向量 `x`。
*   **反向传播**: 当梯度从下一层传回来时，Switch Transformer 层完成自己的参数更新后，会将梯度继续传递给上一层（自注意力层），告诉它应该如何调整，以便下次能提供一个“更好”的 `x`。

---

#### **2. 路由 (Route)**
*   **前向传播**: 路由器（一个权重矩阵 `Wr`）计算该 token `x` 与所有 `N` 个专家的匹配分数（logits），`h(x) = Wr * x`。然后通过 `softmax` 函数得到最终的门控值（概率分布）`p(x)`。
*   **反向传播**: 这是**梯度流的关键分叉点**。梯度会从两个方向汇集到路由器 `Wr`：
    *   **路径一（来自“加权”步骤）**: 这是主要的梯度来源。因为最终输出乘以了门控值 `p(x)`，所以梯度会直接流回 `p(x)`，并进一步通过 `softmax` 和 `h(x)` 的求导法则，计算出对路由器权重 `Wr` 的梯度。这个梯度告诉路由器：“你上次给出的这个概率分布 `p(x)` 导致了最终的误差，你需要这样调整你的权重 `Wr`，以便下次能给出一个更优的概率分布。”
    *   **路径二（来自“负载均衡损失”）**: Switch Transformer 还有一个独立的**辅助损失函数**，专门用来惩罚不均衡的路由。这个辅助损失也会产生梯度，直接作用于 `Wr`。这个梯度告诉路由器：“你上次的分配方案导致专家负载不均，你需要这样调整 `Wr`，以便下次能把任务分配得更均匀一些。”
    *   **最终更新**: 路由器 `Wr` 的总梯度是这两个路径梯度的加和。

---

#### **3. 选择 (Select)**
*   **前向传播**: 根据“Top-1”原则，选择分数最高的那个唯一的专家（例如，专家 `j`）。
*   **反向传播**: 这一步本身是一个简单的 `argmax` 操作，通常被认为是**不可导的**，或者说它的导数处处为零。因此，**梯度不会直接通过“选择”这个决策过程本身**。梯度的流动绕过了这个离散的选择步骤，通过下面描述的路径进行。

---

#### **4. 分发 (Dispatch)**
*   **前向传播**: 将 token 向量 `x` **仅**发送给被选中的专家 `j`。其他所有专家（`i != j`）处于休眠状态。
*   **反向传播**: 这是**梯度流的关键汇集点**。因为在前向传播时只有专家 `j` 被激活了，所以**只有专家 `j` 会接收到从后续步骤传回的梯度**。其他所有休眠的专家（`i != j`）的梯度为零，它们的参数在这次反向传播中**不会被更新**。这正是“稀疏更新”的体现。

---

#### **5. 计算 (Compute)**
*   **前向传播**: 被选中的专家 `j` 对 token 向量 `x` 进行密集的 FFN 计算（`Ej(x)`），得到输出。
*   **反向传播**: 梯度从“加权”步骤传回到专家 `j` 的输出。然后，梯度会根据链式法则，在专家 `j` 的内部（它的线性层和激活函数）进行反向传播，计算出对专家 `j` 所有参数（权重和偏置）的梯度。这些梯度将用于更新专家 `j` 的参数，告诉它：“你上次对输入 `x` 的处理导致了最终的误差，你需要这样调整自己，以便下次能做得更好。”

---

#### **6. 加权 (Weight)**
*   **前向传播**: 将专家 `j` 的输出 `Ej(x)` 乘以其对应的门控值 `pj(x)`，得到该 token 的最终输出 `y = pj(x) * Ej(x)`。
*   **反向传播**: 这是**梯度流的第一个关键分叉点**。根据乘法求导法则，梯度会从 `y` 分成两路：
    *   **一路流向专家 `j` 的输出 `Ej(x)`**: 梯度大小为 `pj(x)`。这个梯度将继续反向传播到专家 `j` 的内部（回到第5步）。
    *   **另一路流向门控值 `pj(x)`**: 梯度大小为 `Ej(x)`。这个梯度将继续反向传播到路由器（回到第2步）。
    *   **这一步是连接“计算路径”和“路由路径”的桥梁，是整个 MoE 能够被端到端训练的核心机制。**

---

#### **7. 返回 (Return)**
*   **前向传播**: 将加权后的输出 `y` 传递给模型的下一层。
*   **反向传播**: 梯度从 `y` 开始，启动了在 Switch Transformer 层内部的整个反向传播过程。

### **总结与洞察**

将正向与反向过程结合起来，我们可以得到几个关键洞察：

1.  **稀疏更新 (Sparse Update)**: 在任何一次迭代中，**只有被激活的那个专家和路由器会被更新**。模型总参数量虽大，但每次更新的参数量很小，这极大地提升了训练效率。
2.  **路由器的学习来源**: 路由器之所以能学会如何“调度”，是因为它同时接收两种“反馈信号”：一种是关于**“任务完成质量”**的（来自主任务的梯度），另一种是关于**“资源分配效率”**的（来自负载均衡的梯度）。
3.  **“加权”步骤是核心**: `y = pj(x) * Ej(x)` 这个简单的乘法，是让梯度能够同时流向“干活的”（专家）和“派活的”（路由器）的关键，它将两个独立的模块无缝地连接在了一起。
4.  **离散选择的绕过**: 梯度并没有尝试去“微分”那个离散的“选择”动作，而是通过对选择结果（门控值 `p(x)`）进行加权，巧妙地将学习信号传递给了决策者（路由器）。



## Switch Transformer 相对于标准 Transformer 的关键改进点

---

### **核心改进点：替换前馈网络 (FFN)**

Switch Transformer 的唯一结构性改动，就是将标准 Transformer 块中的**密集前馈网络 (Dense Feed-Forward Network, FFN) 层**，替换为了一个**稀疏的、由多个专家组成的 Switch FFN 层**。

*   **标准 Transformer**: 只有一个 FFN 层。所有 token 都必须经过这个唯一的 FFN。
*   **Switch Transformer**: 有一个包含 `N` 个专家 FFN 的“专家池”。每个 token 只被路由到**其中一个**专家 FFN 进行计算。

这个看似简单的“替换”动作，是所有后续优势的源头。它从根本上改变了模型的**计算范式**和**扩展方式**。

---

### **由核心改进引发的连锁变化**

这个核心替换，带来了以下几个层面的深刻变革：

#### **1. 计算范式：从“密集计算”到“稀疏激活”**

*   **标准 Transformer (密集)**:
    *   **工作模式**: 全员参与。对于任何输入，模型的所有参数（权重）都参与计算。
    *   **后果**: 参数量和计算量是**强耦合**的。模型参数翻倍，计算成本也几乎翻倍。

*   **Switch Transformer (稀疏)**:
    *   **工作模式**: 条件计算。对于任何一个 token，只有一小部分参数（被选中的那个专家 + 路由器）被“激活”并参与计算。
    *   **后果**: 参数量和计算量被**解耦**。可以拥有巨大的总参数量（所有专家的总和），但单次计算成本却保持在很低的水平（仅相当于一个小型 FFN 的计算量）。

#### **2. 模型扩展性：引入“专家并行”**

*   **标准 Transformer**:
    *   **扩展方式**: 主要通过增加层数、扩大隐藏层维度、增加注意力头数来扩展。当模型变得巨大时，通常需要复杂的模型并行策略（如张量并行、流水线并行）来切分模型。

*   **Switch Transformer**:
    *   **扩展方式**: 引入了一种全新的、极其优雅的扩展维度——**增加专家的数量**。
    *   **专家并行 (Expert Parallelism)**: 这是一种天然的模型并行方式。可以非常简单地将不同的专家分配到不同的计算设备（GPU/TPU）上。想让模型变大？增加更多的设备，并在每个设备上放置新的专家即可。模型的规模可以随着硬件集群的规模**线性增长**。

#### **3. 训练机制：引入“路由”与“负载均衡”**

*   **标准 Transformer**:
    *   **训练**: 端到端的标准反向传播。

*   **Switch Transformer**:
    *   **引入新模块**: 必须增加一个**路由器 (Router)** 模块，负责决定哪个 token 去哪个专家。
    *   **引入新挑战**: 必须解决路由可能导致的**负载不均衡**问题。
    *   **引入新机制**: 必须在总损失函数中加入一个**辅助的负载均衡损失 (Load Balancing Loss)**，以确保所有专家都能得到均匀的训练，从而保证训练的稳定性和效率。

---

### **总结对比表：Switch Transformer vs. 标准 Transformer**

| 特性维度 | 标准 Transformer (Dense Model) | Switch Transformer (Sparse MoE Model) | 核心改进与优势 |
| :--- | :--- | :--- | :--- |
| **核心架构** | 自注意力层 + **单一、密集的 FFN 层** | 自注意力层 + **包含多个专家的稀疏 Switch FFN 层** | **【根本性改进】** 将计算核心 FFN 替换为稀疏专家池。 |
| **计算范式** | **密集计算** (所有参数参与计算) | **稀疏激活 / 条件计算** (仅激活一个专家) | **【效率革命】** 解耦了参数量与计算量，实现了“大模型、低成本”。 |
| **参数与计算** | 参数量与计算量**强耦合** | 参数量与计算量**解耦** | 可以在保持计算成本不变的情况下，将模型总参数量提升数个数量级。 |
| **模型扩展方式** | 增加层数、宽度 (模型并行复杂) | **增加专家数量** (天然的专家并行) | **【优雅的扩展性】** 提供了一种极其简单且高效的规模化路径。 |
| **训练机制** | 标准的反向传播 | 标准反向传播 + **路由机制** + **负载均衡损失** | **【新的训练动力学】** 引入了路由和负载均衡，使训练更复杂但可控。 |
| **硬件利用** | 数据并行、模型并行 | 数据并行、**专家并行** | 完美契合大规模分布式集群，每个设备负责一部分专家，负载可控。 |
| **模型能力来源** | 更深、更宽的密集层带来的拟合能力 | **巨大的参数容量**带来的知识存储能力和泛化能力 | 通过“虚胖”的参数量存储了更丰富的知识，潜力更大。 |

**结论：**

Switch Transformer 的所有改进都源于对 **FFN 层的稀疏化改造**。这一“点”上的突破，带来了计算范式、模型扩展性和训练机制上的“面”的革新，最终使得构建和训练**万亿级别**的、同时保持计算高效的语言模型成为可能，是大型语言模型发展史上一次里程碑式的飞跃。
