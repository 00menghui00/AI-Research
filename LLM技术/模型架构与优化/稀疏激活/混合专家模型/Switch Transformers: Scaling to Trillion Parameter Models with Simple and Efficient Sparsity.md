# Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity(2021)
- 论文地址：https://ar5iv.labs.arxiv.org/html/2101.03961?_immersive_translate_auto_translate=1

## 概述：
- 我们简化了 MoE 路由算法，并设计了通信和计算成本更低、更直观的改进模型。
- 我们提出的训练技术缓解了不稳定性问题，并且首次证明了大型稀疏模型可以用较低精度的（bfloat16）格式进行训练。
- Switch Transformers 的指导设计原则是以简单且计算高效的方式最大化 Transformer 模型的参数数量。（规模效应由openai提出，并证明扩大模型比扩充数据更有性价比）
- 我们的假设是，参数数量，无论总计算量如何，都是可单独扩展的重要维度。（我们可以将参数量 (N) 和 单样本计算量 (FLOPs/sample) 进行解耦 (decouple)。我们可以做到让参数量 (N) 疯狂增长，但处理每个样本（token）所需的计算量却保持不变。）
- 分布式训练：我们的工作重点在于 TPU 架构，但这类模型也可以在 GPU 集群上进行类似训练。在我们的分布式训练设置中，我们的稀疏激活层在不同的设备上分割唯一的权重。因此，模型的权重随着设备数量的增加而增加，同时每个设备上的内存和计算开销保持可控。

## MoE架构：
- Expert：现代基于 Transformer 的 MoE 架构（如 Switch Transformer, Mixtral）中，“专家” (Expert) 指的就是一个标准的前馈网络 (Feed-Forward Network, FFN)。
- 稀疏激活：对于每一个传入的 token，系统并不会激活并使用所有的专家 FFN，而是通过一个路由机制（Router），只选择一个或少数几个（例如，Switch Transformer 中只选1个，Mixtral 中选2个）最相关的专家 FFN 来参与计算。

### 针对FFN做稀疏激活的原因：
- 参数量占比高: 在一个标准的 Transformer 模型中，FFN 层的参数量通常占到总参数量的 2/3 甚至更多。因此，对 FFN 进行“稀疏化”是提升模型参数/计算比（即“性价比”）最有效的地方，能带来最大的收益。
- 计算的独立性: FFN 层对序列中的每个 token 是独立计算的。Token "A" 在 FFN 中的计算与 Token "B" 无关。这种天然的独立性使得为每个 token 单独选择专家的“路由”机制变得非常简单和高效，极易并行化。
- 功能上的合理性: 我们可以直观地将不同的专家 FFN 理解为模型学习到的不同“知识领域”或“处理技能”。例如，一个专家可能擅长处理语法结构，另一个专家擅长处理事实性知识，还有一个专家擅长处理代码逻辑。让路由器根据 token 的内容为其分配合适的专家，在功能上非常合理。




## **Switch Transformer 层处理一个 Token 的完整生命周期（前向 + 反向传播）**

假设我们正在处理一个 token `x`，并且模型预测的最终结果与真实标签之间产生了误差（Loss）。现在，这个误差信号需要从模型的输出层一路传回来，对路径上的所有参数进行调整。

---

#### **1. 接收 (Receive)**
*   **前向传播**: Switch Transformer 层接收来自上一层（例如自注意力层）的输出向量 `x`。
*   **反向传播**: 当梯度从下一层传回来时，Switch Transformer 层完成自己的参数更新后，会将梯度继续传递给上一层（自注意力层），告诉它应该如何调整，以便下次能提供一个“更好”的 `x`。

---

#### **2. 路由 (Route)**
*   **前向传播**: 路由器（一个权重矩阵 `Wr`）计算该 token `x` 与所有 `N` 个专家的匹配分数（logits），`h(x) = Wr * x`。然后通过 `softmax` 函数得到最终的门控值（概率分布）`p(x)`。
*   **反向传播**: 这是**梯度流的关键分叉点**。梯度会从两个方向汇集到路由器 `Wr`：
    *   **路径一（来自“加权”步骤）**: 这是主要的梯度来源。因为最终输出乘以了门控值 `p(x)`，所以梯度会直接流回 `p(x)`，并进一步通过 `softmax` 和 `h(x)` 的求导法则，计算出对路由器权重 `Wr` 的梯度。这个梯度告诉路由器：“你上次给出的这个概率分布 `p(x)` 导致了最终的误差，你需要这样调整你的权重 `Wr`，以便下次能给出一个更优的概率分布。”
    *   **路径二（来自“负载均衡损失”）**: Switch Transformer 还有一个独立的**辅助损失函数**，专门用来惩罚不均衡的路由。这个辅助损失也会产生梯度，直接作用于 `Wr`。这个梯度告诉路由器：“你上次的分配方案导致专家负载不均，你需要这样调整 `Wr`，以便下次能把任务分配得更均匀一些。”
    *   **最终更新**: 路由器 `Wr` 的总梯度是这两个路径梯度的加和。

---

#### **3. 选择 (Select)**
*   **前向传播**: 根据“Top-1”原则，选择分数最高的那个唯一的专家（例如，专家 `j`）。
*   **反向传播**: 这一步本身是一个简单的 `argmax` 操作，通常被认为是**不可导的**，或者说它的导数处处为零。因此，**梯度不会直接通过“选择”这个决策过程本身**。梯度的流动绕过了这个离散的选择步骤，通过下面描述的路径进行。

---

#### **4. 分发 (Dispatch)**
*   **前向传播**: 将 token 向量 `x` **仅**发送给被选中的专家 `j`。其他所有专家（`i != j`）处于休眠状态。
*   **反向传播**: 这是**梯度流的关键汇集点**。因为在前向传播时只有专家 `j` 被激活了，所以**只有专家 `j` 会接收到从后续步骤传回的梯度**。其他所有休眠的专家（`i != j`）的梯度为零，它们的参数在这次反向传播中**不会被更新**。这正是“稀疏更新”的体现。

---

#### **5. 计算 (Compute)**
*   **前向传播**: 被选中的专家 `j` 对 token 向量 `x` 进行密集的 FFN 计算（`Ej(x)`），得到输出。
*   **反向传播**: 梯度从“加权”步骤传回到专家 `j` 的输出。然后，梯度会根据链式法则，在专家 `j` 的内部（它的线性层和激活函数）进行反向传播，计算出对专家 `j` 所有参数（权重和偏置）的梯度。这些梯度将用于更新专家 `j` 的参数，告诉它：“你上次对输入 `x` 的处理导致了最终的误差，你需要这样调整自己，以便下次能做得更好。”

---

#### **6. 加权 (Weight)**
*   **前向传播**: 将专家 `j` 的输出 `Ej(x)` 乘以其对应的门控值 `pj(x)`，得到该 token 的最终输出 `y = pj(x) * Ej(x)`。
*   **反向传播**: 这是**梯度流的第一个关键分叉点**。根据乘法求导法则，梯度会从 `y` 分成两路：
    *   **一路流向专家 `j` 的输出 `Ej(x)`**: 梯度大小为 `pj(x)`。这个梯度将继续反向传播到专家 `j` 的内部（回到第5步）。
    *   **另一路流向门控值 `pj(x)`**: 梯度大小为 `Ej(x)`。这个梯度将继续反向传播到路由器（回到第2步）。
    *   **这一步是连接“计算路径”和“路由路径”的桥梁，是整个 MoE 能够被端到端训练的核心机制。**

---

#### **7. 返回 (Return)**
*   **前向传播**: 将加权后的输出 `y` 传递给模型的下一层。
*   **反向传播**: 梯度从 `y` 开始，启动了在 Switch Transformer 层内部的整个反向传播过程。

### **总结与洞察**

将正向与反向过程结合起来，我们可以得到几个关键洞察：

1.  **稀疏更新 (Sparse Update)**: 在任何一次迭代中，**只有被激活的那个专家和路由器会被更新**。模型总参数量虽大，但每次更新的参数量很小，这极大地提升了训练效率。
2.  **路由器的学习来源**: 路由器之所以能学会如何“调度”，是因为它同时接收两种“反馈信号”：一种是关于**“任务完成质量”**的（来自主任务的梯度），另一种是关于**“资源分配效率”**的（来自负载均衡的梯度）。
3.  **“加权”步骤是核心**: `y = pj(x) * Ej(x)` 这个简单的乘法，是让梯度能够同时流向“干活的”（专家）和“派活的”（路由器）的关键，它将两个独立的模块无缝地连接在了一起。
4.  **离散选择的绕过**: 梯度并没有尝试去“微分”那个离散的“选择”动作，而是通过对选择结果（门控值 `p(x)`）进行加权，巧妙地将学习信号传递给了决策者（路由器）。

