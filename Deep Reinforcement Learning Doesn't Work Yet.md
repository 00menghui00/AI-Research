# Deep Reinforcement Learning Doesn't Work Yet
原文地址：https://www.alexirpan.com/2018/02/14/rl-hard.html

## 概念扫盲

### 1. Arcade Learning Environment (ALE)
   ALE是一个软件接口，它提供了一个统一的平台，允许强化学习算法与Atari 2600游戏进行交互。研究人员可以通过ALE获取游戏的原始像素画面、得分、游戏状态等信息，并向游戏发送动作指令。

### 2.DQN
  DQN的创新之处在于，它使用深度神经网络来近似Q函数，从而能够处理高维度的输入（例如游戏画面的原始像素），并输出每个可能动作的Q值。Q-learning 是一种经典的强化学习算法，它通过学习一个Q函数（Q-value function），来估计在给定状态下采取某个动作所能获得的未来累积奖励。Q函数通常用表格来表示（Q-table），但在状态空间或动作空间非常大的情况下，这种方法变得不可行。

### 3.MuJoCo
   MuJoCo 全称是 Multi-Joint dynamics with Contact，它是一个由 Roboti LLC 开发的物理模拟器。它最初是为机器人控制、生物力学和图形动画等领域设计的。

### 4.蒙特卡洛树搜索 (MCTS)
- 蒙特卡洛树搜索（MCTS） 是一种用于决策过程的启发式搜索算法，它结合了树搜索的精确性和蒙特卡洛模拟的随机性。它特别适用于具有巨大搜索空间（例如围棋、国际象棋等）的复杂游戏。
- MCTS 的核心思想是通过**模拟（rollout）**来评估当前状态下不同行动的潜在价值，而不是像传统搜索算法那样进行穷举搜索。它通过在搜索树中进行多次模拟来逐步构建和改进搜索树。
- UCT（Upper Confidence Bound 1 applied to Trees） 是一种在 MCTS 的**选择（Selection）阶段使用的特定策略。它是 MCTS 成功的关键因素之一，因为它有效地解决了在搜索树中探索（Exploration）和利用（Exploitation）**之间的平衡问题。

## 核心观点

### 深度强化学习的样本效率极低
- 计划谬误是指完成某件事通常比你预想的要花更长的时间。强化学习也存在其自身的计划谬误——学习一项策略通常需要比你预想的更多的样本。

### 如果你只关心最终性能，很多问题可以通过其他方法更好地解决
- 在寻找任何研究问题的解决方案时，通常需要在不同目标之间进行权衡。你可以优化以获得该研究问题的真正好的解决方案，也可以优化以获得出色的研究贡献。最好的问题是那些需要做出出色的研究贡献才能获得好的解决方案的问题，但找到符合该标准的可行问题可能很困难。单纯从获得良好性能的角度来看，深度强化学习的以往记录并不出色，因为它总是被其他方法超越。

- 强化学习理论上可以应用于任何环境，包括未知世界模型的环境。然而，这种通用性是有代价的：它很难利用任何可能有助于学习的特定问题信息，这迫使你使用大量样本来学习那些原本可以硬编码的内容。

- 经验法则是，除了极少数情况外，特定领域算法通常比强化学习更快、更好。如果你只是为了深度强化学习而进行深度强化学习，这倒不成问题，但当我将强化学习的性能与其他任何技术进行比较时，我个人觉得很沮丧。

### 强化学习通常需要奖励函数
- 强化学习假设存在一个奖励函数。通常，奖励函数要么是给定的，要么是离线手动调整的，并在学习过程中保持不变。我之所以说“通常”，是因为存在例外，例如模仿学习或逆强化学习，但大多数强化学习方法都将奖励视为一个预言机。

- 重要的是，为了让强化学习正确发挥作用，你的奖励函数必须准确捕捉你想要的结果。我的意思是准确捕捉。强化学习有一个恼人的倾向，那就是过度拟合你的奖励函数，导致你意想不到的结果。

### 奖励函数设计很困难
- 制作奖励函数并不难。难就难在如何设计一个既能鼓励你想要的行为，又能保持可学习的奖励函数。

- 形状奖励是指在接近最终目标的状态下会给予越来越多的奖励。这与稀疏奖励不同，稀疏奖励在目标状态下给予奖励，在其他状态下则不给予奖励。形状奖励通常更容易学习，因为即使策略尚未找到问题的完整解决方案，它们也会提供积极的反馈。

- 不幸的是，形状奖励可能会对学习造成偏差。如前所述，这可能会导致与预期不符的行为。一个很好的例子是赛艇游戏，来自OpenAI 的一篇博客文章（https://blog.openai.com/faulty-reward-functions/）。

- 强化学习只会看得到奖励的结果而不关心实现过程，如果采用形状奖励可能导致学习到的过程是有偏差的，但如果使用稀疏奖励则会因为缺乏正向强化（正确的过程很难学习）让一切变得很困难。解决这个问题的另一种方法是仔细地调整奖励机制，添加新的奖励项，并调整现有奖励项的系数，直到你想要学习的行为能够被强化学习算法识别出来。在这方面，对抗强化学习是可能的，但这种对抗非常不令人满意。有时，这样做是必要的，但我从未觉得从中学到什么。

### 即使给予良好的奖励，局部最优也很难突破
- 经典的探索-利用问题，这个问题自古以来就困扰着强化学习。你的数据来自你当前的策略。如果你当前的策略探索过度，你得到的就是垃圾数据，什么也学不到。利用过度，你就会养成一些不太理想的行为。

### 即使深度强化学习有效，也可能只是对环境中的奇怪模式过度拟合——难以泛化
- 强化学习的优点在于，如果你想在某个环境中表现良好，你可以肆意地过度拟合。缺点在于，如果你想推广到其他任何环境，你很可能表现不佳，因为你疯狂地过度拟合。

- 多智能体强化学习的一个持续主题：当智能体相互对抗训练时，会发生一种共同进化。智能体非常擅长互相对抗，但当它们被部署到与未知玩家对抗时，性能就会下降。

### 即使忽略泛化问题，最终结果也可能不稳定且难以重现
- 机器学习为失败案例增加了更多维度，从而成倍增加了失败的可能性。深度强化学习则增加了一个新的维度：随机性。而解决随机性的唯一方法是针对问题进行足够多的实验，以消除噪音。当你的训练算法既样本效率低下又不稳定时，它会严重减慢你的研究效率。也许只需要 100 万步。但当你将其乘以 5 个随机种子，然后再乘以超参数调整时，你需要大量的计算才能有效地测试假设。

- 将奖励乘以一个常数可能会导致表现的显著差异。
- 五个随机种子（常见的报告指标）可能不足以论证显著的结果，因为通过仔细选择，您可以获得不重叠的置信区间。
- 即使使用相同的超参数，同一算法的不同实现在同一任务上也会有不同的性能。

- 强化学习对初始化和训练过程的动态都非常敏感，因为你的数据始终是在线收集的，而且你获得的唯一监督只有一个奖励标量。一个随机发现良好训练样本的策略会比没有发现的策略更快地自我引导。一个未能及时发现良好训练样本的策略会崩溃，最终什么也学不到，因为它会更加确信它尝试的任何偏差都会失败。

### 鉴于目前的限制，深度强化学习何时能为我所用？
- 我们可以找出一些使学习更容易的常见属性。以下属性并非学习的必需，但满足其中更多的属性无疑会更好。
  1. 生成近乎无限量的经验。这其中的益处显而易见。数据越多，学习问题就越容易。
  2. 问题简化成更简单的形式。我在深度强化学习中看到的一个常见错误就是梦想太大。强化学习无所不能！但这并不意味着你必须一次性完成所有事情。
  3. 有一种方法可以将自我对弈引入学习。需要注意的是，我所说的自我对弈，指的是游戏具有竞争性，并且两个玩家都可以由同一个智能体控制的场景。到目前为止，这种场景似乎具有最稳定、性能最佳的行为。
  4. 有一种清晰的方法可以定义可学习、不可游戏的奖励。任何时候引入奖励塑造，都意味着学习到非最优策略，从而优化错误的目标。
  5. 如果要塑造奖励，至少应该足够丰厚。行动和结果之间的延迟越短，反馈回路闭合得越快，强化学习就越容易找到通往高奖励的路径。
