# Group Relative Policy Optimization (GRPO)
- 介绍文章：https://www.oxen.ai/blog/why-grpo-is-important-and-how-it-works
- 提出论文：https://arxiv.org/html/2402.03300?_immersive_translate_auto_translate=1

## 概述：
1. GRPO（训练deepseek）相比PPO（训练chatgpt）显著降低计算需求（gpu算力需求）。（在 PPO 中，策略模型和值模型都有需要通过反向传播进行训练的参数。反向传播需要大量的内存。GRPO 省略了值模型。）
2. 




---


## GRPO与PPO对比：

### PPO 的“痛点”：昂贵的价值模型 (Value Model)

在 PPO 算法中，整个系统通常包含两个需要训练的大型模型：

1.  **策略模型 (Policy Model)**：这是我们最终想要得到的模型。它的任务是生成回答（token序列）。
2.  **价值模型 (Value Model)**：这是一个“辅助”模型。它的任务是**预测**策略模型生成的回答未来能获得多高的奖励。它就像一个批评家，在旁边不停地对策略模型的表现进行估价。

**为什么需要价值模型？**
PPO 需要计算“优势”（Advantage），即 `优势 = 实际奖励 - 预期奖励`。这个“预期奖励”就是由价值模型来提供的。通过知道一个回答是“比预期的好”还是“比预期的差”，PPO 可以更稳定地更新策略模型。

**痛点在哪里？**
这个价值模型本身也通常是一个和策略模型差不多大的大型语言模型（LLM）。这意味着：
*   **训练成本加倍**：您需要同时训练和更新两个巨大的模型，消耗大量的计算资源和时间。
*   **内存需求加倍**：在训练时，需要将两个大模型都加载到GPU内存中，对硬件要求极高。
*   **训练不稳定**：价值模型的训练本身也可能不稳定，一个不准确的价值模型会提供错误的“预期奖励”，从而误导策略模型的学习。

### GRPO 的解决方案：用“样本”代替“预测”

GRPO 的核心思想是：“**我能不能不'预测'平均奖励，而是直接通过采样来'估计'一个平均奖励呢？**”

这就是您引用的“第一个技巧”发挥作用的地方：

> **"GRPO 不是针对每个查询生成一个输出，而是首先生成多个输出。"**

这个简单的改变带来了根本性的变化。下面是 GRPO 的运作方式：

1.  **生成多个候选答案 (Generate Multiple Outputs)**
    对于同一个用户问题（Query），GRPO 会让当前的策略模型生成 `N` 个不同的回答（比如，生成 4 个或 8 个）。我们把这些回答称为一个“批次”或“样本集”。

2.  **为每个答案评分 (Get Rewards)**
    接下来，一个固定的、不需要训练的奖励模型（Reward Model）会为这 `N` 个回答中的每一个都打一个分数。
    *   回答1 -> 奖励 R1
    *   回答2 -> 奖励 R2
    *   ...
    *   回答N -> 奖励 RN

3.  **计算样本平均奖励 (Calculate Sample Mean Reward)**
    现在，GRPO 有了这 `N` 个具体的奖励分数。它可以直接计算出这批样本的**平均奖励**：
    `平均奖励 (Baseline) = (R1 + R2 + ... + RN) / N`

4.  **用样本平均值代替价值模型的预测**
    这是最关键的一步。GRPO 直接用这个刚刚计算出的**样本平均奖励**来充当 PPO 中那个需要复杂价值模型才能预测出的“预期奖励”（也称为基线 Baseline）。

    现在，对于这批样本中的**每一个回答**，GRPO 都可以计算出一个类似“优势”的值：
    *   回答1的“优势” ≈ `R1 - 平均奖励`
    *   回答2的“优势” ≈ `R2 - 平均奖励`
    *   ...

5.  **更新策略模型**
    *   如果一个回答的奖励**高于**这批样本的平均奖励（“优势”>0），算法就会调整策略模型，增加未来生成类似回答的概率。
    *   如果一个回答的奖励**低于**平均奖励（“优势”<0），就降低生成类似回答的概率。

### 总结：GRPO 如何消除价值模型的需求

| 特性 | PPO (Proximal Policy Optimization) | GRPO (Generative Reward Policy Optimization) |
| :--- | :--- | :--- |
| **如何获得“预期奖励”** | 依赖一个**可训练的、大型的价值模型**去**预测**一个预期值。 | 通过**生成多个样本**，然后**计算**这些样本的**实际平均奖励**。 |
| **核心思想** | **预测未来 (Prediction)** | **用样本估计整体 (Estimation by Sampling)** |
| **模型需求** | 1. 策略模型 (LLM) <br> 2. 价值模型 (LLM) | 1. 策略模型 (LLM) <br> (奖励模型是固定的，不算在训练循环内) |
| **优势** | 理论上更精确（如果价值模型完美的话）。 | **移除了价值模型**，极大降低了训练的**计算成本**和**内存需求**，并简化了训练流程。 |

所以，GRPO 的巧妙之处在于，它用一个非常简单、代价低廉的“**多样本采样求平均**”操作，来替代了 PPO 中那个极其昂贵、需要独立训练的“**价值模型预测**”操作。这个改变使得在资源有限的情况下，对大语言模型进行对齐训练变得更加可行和高效。
