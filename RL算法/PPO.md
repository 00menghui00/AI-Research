# Proximal Policy Optimization Algorithms





---
## PPO-Penalty

````markdown
### 1. 核心优化公式

```latex
$$
J(\theta)
= \displaystyle
\mathbb{E}_{\tau\sim\pi_{\theta}}\Bigl[
  R(\tau)
  - \beta \sum_{t=0}^{T}
    D_{\mathrm{KL}}\bigl(
      \pi_{\theta}(a_t\mid s_t)
      \,\big\|\,
      \pi_{\mathrm{ref}}(a_t\mid s_t)
    \bigr)
\Bigr]
$$
````

---

### 2. 公式各项含义解析

#### **等式左边**

* **`J(\theta)` (优化目标)**

  * **含义**：这是我们要最大化的 **“模型综合评分函数”**，它根据当前参数 \$\theta\$ 动态计算得出。
  * **角色**：相当于梯度上升要攀登的“山峰”，是整个优化算法的最终目标。

#### **等式右边**

* **`\mathbb{E}_{\tau\sim\pi_{\theta}}[\dots]` (期望)**

  * **含义**：对当前策略 \$\pi\_{\theta}\$ 生成的所有可能轨迹 \$\tau\$，取括号内内容的平均值。
* **`R(\tau)` (奖励项)**

  * **含义**：一个预训练的奖励模型对完整轨迹 \$\tau\$ 给出的质量分数。
  * **角色**：驱动模型改进的“油门”，告诉模型什么样的回答是“好”的。
* **`\beta` (Beta 系数)**

  * **含义**：超参数，用来平衡奖励和KL散度惩罚的权重。
  * **角色**：协调“油门”和“刹车”力度。
* **`\sum_{t=0}^{T} D_{\mathrm{KL}}(\dots)` (KL散度惩罚)**

  * **含义**：对生成过程每一步 \$t\$，计算当前模型 \$\pi\_{\theta}\$ 与参考模型 \$\pi\_{\mathrm{ref}}\$ 输出分布间的 KL 散度，并累加至总和。
  * **角色**：防止模型偏离参考策略过远的“刹车”或“安全绳”。

---

### 3. 各项在训练流程中的动态变化

| 组件                                     | 初始状态 (t=0)             | 训练中变化趋势                 |
| :------------------------------------- | :--------------------- | :---------------------- |
| **`J(\theta)` (优化目标)**                 | 初始基准分数                 | 在奖励与惩罚的共同作用下**持续上升**    |
| **`R(\tau)` (奖励项)**                    | 初始模型回答质量相对较低           | 随模型学习生成更高质量回答而**逐渐上升**  |
| **`D_{\mathrm{KL}}(\dots)` (KL散度惩罚项)** | **0**（新模型 = 参考模型，散度为零） | 随模型更新偏离参考策略而**逐渐增大**    |
| **`\beta` (Beta 系数)**                  | 固定正数                   | 通常**保持不变**，或在高级策略下自适应调整 |

---

### 4. 完整的单次训练迭代过程

以下是一轮“**生成 → 评估 → 求梯度 → 更新**”的完整步骤：

1. **采样与生成（Forward Pass – Part 1）**

   1. 从数据集中随机抽取 \$N\$ 个用户提问（Prompts）。
   2. 用当前策略 \$\pi\_{\theta}\$ 对每个 Prompt 进行 Rollout，生成完整回答（轨迹 \$\tau\$）。

2. **评估与计算（Forward Pass – Part 2）**
   3\. **计算奖励**：将每个 “Prompt + Response” 对输入固定奖励模型 \$R\$，得到 \$R(\tau)\$。
   4\. **计算 KL 散度**：在生成每一步时，分别取 \$\pi\_{\theta}\$ 和 \$\pi\_{\mathrm{ref}}\$ 的输出概率，计算 KL 散度并累加，得出该轨迹的总值。
   5\. **组合目标**：应用公式将奖励与 KL 散度惩罚结合，算出这批样本的 \$J(\theta)\$。

3. **计算梯度（Backward Pass）**
   6\. 调用自动求导（如 `objective.backward()`）。
   7\. 通过反向传播与链式法则，得到 \$\nabla J(\theta)\$。

4. **更新模型参数（Optimizer Step）**
   8\. 优化器（如 Adam）获取 \$\nabla J(\theta)\$。
   9\. 依据更新规则（如 \$\theta \leftarrow \theta + \eta \nabla J(\theta)\$）进行参数微调。

迭代结束后，模型性能略有提升，然后进入下一次循环，直至收敛。

```
```
