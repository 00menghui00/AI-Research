# Lost in the Middle: How Language Models Use Long Contexts(2023)
- 论文地址：https://ar5iv.labs.arxiv.org/html/2307.03172?_immersive_translate_auto_translate=1

## 一、abstract：
- 即使LLM能处理成千上万token量的长文本，但是LLM并不一定能完全理解这些信息
- LLM表现 高度依赖相关信息在文本中的位置。如果信息被放在不同位置（开头、中间、结尾），模型性能变化明显。说明模型并不是“全局理解”文本，而是对位置敏感，缺乏对长文本的稳定利用能力。
